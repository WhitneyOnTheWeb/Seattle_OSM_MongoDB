{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Wrangle OpenStreetMap Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greater Seattle Region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Whitney King"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project will wrangle OpenStreetMap data pertaining to the Greater Seattle region in Washington State, USA. Seattle was chosen, as it's my home city, and it will be interesting to work with data that contains locations I'm personally familiar with. The dataset was obtained from a preselected region of OSM data hosted by MapZen:\n",
    "https://mapzen.com/data/metro-extracts/metro/seattle_washington/\n",
    "\n",
    "The main objective of this project will be to ensure that pertinent dirty data has been cleaned up and corrected prior to converting the data to JSON. Data wrangling will be done in MongoDB using Python3. We'll begin by importing modules that may come in handy, as well as setting up variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset for the Greater Seattle Region is extremely large (1.6 GB), we'll first generate a sample of the data set to assist with preliminary development and analysis. By getting a sample of the data that is less than 100MB, we'll still have a good chunk of the data from the region, but processing time will be significantly less. For the purposes of this stage of the data wrangling, this sample set will work just fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seattle_small_sample.osm created:\n",
      "File size:  1.8 MB\n"
     ]
    }
   ],
   "source": [
    "# Python Modules\n",
    "import xml.etree.cElementTree as xET\n",
    "from collections import defaultdict\n",
    "import csv\n",
    "import os\n",
    "import pprint\n",
    "import re\n",
    "import codecs\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# OSM Files\n",
    "OSM_NAME = \"seattle_washington.osm\"\n",
    "OSM_FILE = open(OSM_NAME, \"rb\")\n",
    "\n",
    "SAMPLE_NAME = \"seattle_sample.osm\"  # k = 30\n",
    "SAMPLE_FILE = open(SAMPLE_NAME, \"rb\")\n",
    "\n",
    "SMALL_SAMPLE_NAME = \"seattle_small_sample.osm\"  # k = 900\n",
    "SMALL_SAMPLE_FILE = open(SMALL_SAMPLE_NAME, \"rb\")\n",
    "\n",
    "# Street Types in Addresses\n",
    "st_types = defaultdict(set)\n",
    "\n",
    "\n",
    "# Paramenter: k-th top level element\n",
    "k = 900  # Larger number, small sample\n",
    "\n",
    "def get_element(OSM_NAME, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = xET.iterparse(OSM_NAME, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def create_sample():\n",
    "    with open(SMALL_SAMPLE_NAME, 'wb') as output:\n",
    "        output.write(bytes('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n', encoding=\"utf-8\"))\n",
    "        output.write(bytes('<osm>\\n  ', encoding=\"utf-8\"))\n",
    "\n",
    "        # Write every 10th top level element\n",
    "        for i, element in enumerate(get_element(OSM_NAME)):\n",
    "            if i % k == 0:\n",
    "                output.write(xET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "        output.write(bytes('</osm>', encoding=\"utf-8\"))\n",
    "        \n",
    "        print(SMALL_SAMPLE_NAME, 'created:')\n",
    "        print('File size: ', file_size(SMALL_SAMPLE_NAME))\n",
    "        \n",
    "       \n",
    "    \n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/2104080/how-to-check-file-size-in-python\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            size = \"%3.1f %s\" % (num, x)\n",
    "            return size\n",
    "        num /= 1024.0\n",
    "        \n",
    "        \n",
    "\n",
    "def file_size(SMALL_SAMPLE_NAME):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/2104080/how-to-check-file-size-in-python\n",
    "    \"\"\"\n",
    "    file_info = os.stat(SMALL_SAMPLE_NAME)\n",
    "    size = convert_bytes(file_info.st_size)\n",
    "    return size\n",
    "\n",
    "create_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Element Tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "After we've imported the OSM XML data, we'll want to take a look at how the elements are broken down. Counting the tags that show up in the dataset will allow us to gain an understanding of the size and structure of the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'member': 3023,\n",
      " 'nd': 287779,\n",
      " 'node': 257664,\n",
      " 'osm': 1,\n",
      " 'relation': 317,\n",
      " 'tag': 157875,\n",
      " 'way': 25509}\n"
     ]
    }
   ],
   "source": [
    "def count_tags(f):\n",
    "    '''\n",
    "    Reference:  Udacity\n",
    "    '''\n",
    "    tags = {}\n",
    "    for ev, elem in xET.iterparse(f):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            tags[tag] = 1\n",
    "        else:\n",
    "            tags[tag] = tags[tag] + 1    \n",
    "    return tags\n",
    "\n",
    "\n",
    "def tag_count():\n",
    "    #Prints off tags found in the OSM XMl file, along with their counts\n",
    "    tags = count_tags(SAMPLE_NAME)\n",
    "    pprint.pprint(tags)\n",
    "    \n",
    "    \n",
    "#May take in excess of several minutes to run due to the size of the data file\n",
    "tag_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List Element Tag Names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to see the counts of occurrences for each type of element tag in the XML (particularly ```node``` and ```way```, as they contain the most interesting fields), however these counts only show a top level list of elements, not the tags/values nested therein. To get a better feel for the information available, we'll generate a list of tags, as well as count their nested fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'node': {'highway': 1468, 'created_by': 2315, 'name': 1200, 'amenity': 763, 'name:en': 27, 'ref': 259, 'crossing': 189, 'button_operated': 1, 'tiger:tzid': 16, 'source': 6828, 'exit_to': 5, 'exit_to:left': 2, 'exit_to:right': 2, 'noref': 4, 'atm': 9, 'power': 1383, 'odbl': 3, 'railway': 184, 'traffic_signals': 7, 'place': 42, 'wikipedia': 5, 'population': 6, 'census:population': 2, 'leisure': 74, 'bicycle': 30, 'level_crossing': 1, 'traffic_calming': 95, 'stop': 3, 'note': 96, 'landuse': 10, 'man_made': 37, 'noexit': 22, 'bus': 101, 'gtfs:stop_id': 203, 'public_transport': 215, 'junction': 13, 'access': 28, 'barrier': 110, 'direction': 24, 'is_in:state_code': 24, 'website': 103, 'ele': 139, 'is_in': 32, 'gnis:id': 51, 'gnis:Class': 51, 'gnis:County': 51, 'gnis:ST_num': 51, 'import_uuid': 31, 'gnis:ST_alpha': 31, 'gnis:County_num': 51, 'addr:city': 3696, 'addr:state': 89, 'wikidata': 3, 'natural': 192, 'name:sal': 1, 'aeroway': 14, 'ref:left': 1, 'ref:right': 1, 'attribution': 1071, 'capacity': 98, 'shop': 274, 'wheelchair': 31, 'is_in:continent': 1, 'gtfs:dataset_id': 113, 'operator': 161, 'foot': 29, 'religion': 23, 'denomination': 13, 'gnis:Cell': 20, 'gnis:ST_alph': 20, 'gnis:created': 92, 'is_in:county': 20, 'gnis:state_id': 77, 'is_in:country': 20, 'gnis:county_id': 77, 'gnis:feature_id': 95, 'tourism': 91, 'waterway': 13, 'gnis:edited': 6, 'city': 1, 'building': 11, 'gnis:county_name': 17, 'source_ref': 10, 'gnis:feature_type': 15, 'historic': 11, 'bollard': 1, 'addr:street': 4161, 'addr:postcode': 2867, 'addr:province': 6, 'internet_access': 14, 'addr:housenumber': 4173, 'barbecue_grill': 1, 'checked_exists:date': 22, 'cuisine': 124, 'tram': 2, 'sport': 4, 'phone': 68, 'kerb': 35, 'tactile_paving': 19, 'dispensing': 1, 'entrance': 53, 'sloped_curb': 1, 'crossing_ref': 8, 'horse': 6, 'bench': 12, 'shelter': 19, 'verified': 1, 'seamark:type': 102, 'military': 5, 'tower:type': 7, 'tower:construction': 3, 'covered': 14, 'stop_id': 18, 'gps:hdop': 1, 'emergency': 10, 'date': 12, 'display': 1, 'support': 1, 'visibility': 1, 'thermometer': 1, 'fixme': 5, 'parking': 14, 'opening_hours': 54, 'fee': 88, 'tents': 1, 'caravans': 1, 'backrest': 19, 'vending': 3, 'office': 60, 'addr:full': 3, 'description': 53, 'addr:unit': 601, 'bicycle_parking': 7, 'seamark:name': 3, 'seamark:light:range': 3, 'seamark:light:colour': 3, 'seamark:light:height': 2, 'seamark:light:category': 1, 'seamark:light:character': 3, 'seamark:light:reference': 2, 'level': 15, 'loc_name': 3, 'bin': 1, 'internet_access:fee': 4, 'information': 15, 'lcn': 2, 'lcn_ref': 1, 'network': 6, 'motorcar': 1, 'motorcycle': 1, 'brand': 17, 'train': 5, 'construction': 2, 'mooring': 2, 'material': 9, 'smoking': 8, 'takeaway': 10, 'outdoor_seating': 10, 'layer': 1, 'fuel:cng': 1, 'fuel:biogas': 1, 'fuel:diesel': 1, 'fuel:biodiesel': 1, 'fuel:octane_91': 1, 'fuel:octane_95': 1, 'fuel:octane_98': 1, 'fuel:GTL_diesel': 1, 'fuel:HGV_diesel': 1, 'fuel:octane_100': 1, 'fuel:electricity': 1, 'supervised': 4, 'crossing:light': 1, 'crossing:barrier': 1, 'source:url': 82, 'source:license': 82, 'sdot:bike_rack:id': 82, 'sdot:bike_rack:type': 82, 'sdot:bike_rack:facility': 82, 'sdot:bike_rack:condition': 82, 'start_date': 24, 'hiking': 1, 'board_type': 4, 'service:bicycle:rental': 1, 'service:bicycle:repair': 1, 'service:bicycle:retail': 1, 'recycling_type': 6, 'addr:country': 5, 'source:addr': 7, 'drive_in': 1, 'disused:amenity': 1, 'opening_hours:url': 1, 'payment:bitcoin': 1, 'craft': 2, 'wetap:status': 1, 'addr:housename': 1, 'sat': 1, 'hdop': 1, 'time': 2, 'surveillance:operator': 3, 'toilets:wheelchair': 1, 'payment:visa': 2, 'payment:mastercard': 2, 'payment:diners_club': 1, 'payment:discover_card': 2, 'payment:american_express': 1, 'surface': 4, 'name:zh': 4, 'delivery': 3, 'motor_vehicle': 3, 'fax': 1, 'email': 4, 'reservation': 1, 'height': 2, 'artwork_type': 18, 'source:addr:id': 128, 'fire_hydrant:type': 1, 'artist_name': 17, 'artist:wikipedia': 1, 'media': 11, 'sac_id': 11, 'address': 10, 'project': 14, 'location': 11, 'collection_times': 1, 'CP': 1, 'local_ref': 7, 'golf': 1, 'water': 1, 'park_ride': 2, 'ele:msl': 1, 'ele:source': 1, 'measurements': 7, 'classification': 1, 'alt_name': 1, 'leaf_type': 6, 'organic': 1, 'lit': 2, 'advertising': 1, 'species': 2, 'species:en': 3, 'leaf_cycle': 2, 'toilets:disposal': 1, 'distance': 2, 'wifi': 1, 'drive_through': 5, 'contact:phone': 1, 'contact:website': 1, 'fuel': 1, 'trolleybus': 1, 'shelter_type': 1, 'drinking_water': 1, 'playground': 1, 'memorial': 3, 'indoor': 4, 'source:id': 1, 'recycling:cans': 2, 'recycling:glass': 2, 'recycling:paper': 2, 'recycling:clothes': 2, 'recycling:plastic': 2, 'recycling:glass_bottles': 2, 'seamark:light:number': 1, 'seamark:light:period': 2, 'seamark:landmark:colour': 1, 'seamark:landmark:category': 1, 'camp_site': 1, 'impromptu': 1, 'camp_pitch:type': 1, 'camp_pitch:drain': 1, 'camp_pitch:electric': 1, 'camp_pitch:drinking_water': 1, 'seamark:radar_transponder:group': 1, 'seamark:radar_transponder:category': 1, 'seamark:buoy_special_purpose:colour': 1, 'manometer': 1, 'cuisine_1': 1, 'camera:mount': 3, 'camera:type': 3, 'surveillance': 3, 'surveillance:type': 3, 'camera:direction': 1, 'ford': 1}, 'tag': {}, 'nd': {}, 'way': {'NHS': 50, 'ref': 274, 'name': 6150, 'lanes': 1157, 'highway': 10117, 'surface': 1034, 'maxspeed': 979, 'source:maxspeed': 34, 'cycleway': 218, 'oneway': 915, 'aeroway': 33, 'created_by': 378, 'foot': 333, 'horse': 89, 'access': 358, 'bicycle': 596, 'railway': 159, 'segregated': 40, 'motor_vehicle': 64, 'lanes:backward': 185, 'lanes:forward': 181, 'turn:lanes:backward': 91, 'turn:lanes:forward': 82, 'width': 103, 'source': 9288, 'abutters': 1, 'reviewed': 26, 'zip_left': 20, 'name_base': 26, 'name_type': 26, 'separated': 27, 'zip_right': 18, 'tiger:cfcc': 4592, 'tiger:tlid': 1935, 'to_address_left': 20, 'to_address_right': 18, 'from_address_left': 20, 'from_address_right': 18, 'name_direction_suffix': 2, 'zip_left_1': 2, 'zip_right_1': 1, 'to_address_left_1': 2, 'to_address_right_1': 1, 'from_address_left_1': 2, 'from_address_right_1': 1, 'service': 1802, 'bridge': 182, 'destination': 28, 'destination:ref': 29, 'layer': 237, 'description': 10, 'hgv': 41, 'sidewalk': 135, 'bus:lanes': 35, 'hov:lanes': 39, 'hov:minimum': 24, 'tiger:county': 4576, 'tiger:reviewed': 3548, 'tiger:zip_left': 3141, 'tiger:name_base': 3854, 'tiger:name_type': 2198, 'tiger:zip_right': 3010, 'tiger:name_direction_prefix': 1019, 'old_ref': 92, 'natural': 430, 'power': 87, 'cables': 43, 'voltage': 51, 'odbl': 6, 'ele': 45, 'place': 21, 'gnis:created': 41, 'gnis:state_id': 39, 'gnis:county_id': 39, 'gnis:feature_id': 51, 'area': 45, 'man_made': 163, 'tiger:source': 1932, 'tiger:separated': 1860, 'tiger:upload_uuid': 494, 'name_1': 321, 'tiger:name_base_1': 353, 'tiger:name_type_1': 173, 'tiger:zip_left_1': 94, 'tiger:zip_right_1': 49, 'note': 27, 'tiger:name_direction_prefix_1': 96, 'name_2': 36, 'tiger:name_base_2': 31, 'tiger:name_type_2': 13, 'tiger:zip_left_2': 18, 'tiger:zip_right_2': 12, 'tiger:name_direction_suffix_1': 98, 'tiger:zip_left_3': 5, 'tiger:zip_right_3': 3, 'tiger:name_direction_suffix': 1922, 'tiger:name_direction_prefix_2': 8, 'tiger:name_direction_suffix_2': 6, 'usage': 39, 'alt_name': 48, 'shoulder': 18, 'shoulder:width': 16, 'tracktype': 26, 'lanes:both_ways': 38, 'turn:lanes:both_ways': 32, 'FIXME': 13, 'lwn': 3, 'addr:postcode': 6069, 'motor_vehicle:lanes': 21, 'is_in': 88, 'postal_code': 22, 'old_name': 18, 'addr:city': 6045, 'addr:street': 6193, 'addr:housenumber': 6185, 'route': 3, 'duration': 1, 'motorcar': 4, 'operator': 47, 'lcn': 14, 'old_name:1875-1895': 3, 'maxspeed:source': 1, 'bus': 15, 'access:lanes': 3, 'turn:lanes': 61, 'city': 9, 'gauge': 27, 'electrified': 28, 'railway:traffic_mode': 16, 'fixme': 83, 'sport': 149, 'leisure': 348, 'sourceref': 3, 'tiger:zip_left_4': 1, 'tiger:zip_right_4': 1, 'hgv:lanes': 4, 'hgv:minweight': 4, 'amenity': 610, 'emergency': 6, 'opening_hours': 31, 'phone': 53, 'website': 90, 'wikidata': 2, 'wikipedia': 14, 'waterway': 1328, 'maxspeed:hgv': 7, 'fee': 24, 'parking': 146, 'landuse': 587, 'water': 48, 'tunnel': 60, 'wheelchair': 25, 'inline_skates': 8, 'hov': 21, 'aerialway': 2, 'aerialway:occupancy': 1, 'building': 10907, 'shop': 89, 'source:name': 14, 'tourism': 18, 'building:levels': 112, 'crossing': 41, 'footway': 213, 'cuisine': 36, 'source:addr:id': 25, 'boat': 9, 'step_count': 1, 'lit': 36, 'basin': 2, 'level': 26, 'indoor': 26, 'addr:state': 155, 'gnis:county_name': 8, 'tiger:name_base_3': 4, 'tiger:name_direction_prefix_3': 1, 'religion': 24, 'denomination': 17, 'gnis:edited': 10, 'substation': 19, 'barrier': 142, 'distance': 5, 'sac_scale': 40, 'attribution': 1122, 'import_uuid': 1080, 'US-NPS:way_id': 5, 'distance_unit': 5, 'NPS:NOW_TRLCSS': 5, 'geobase:uuid': 21, 'geobase:datasetName': 21, 'geobase:acquisitionTechnique': 29, 'wetland': 9, 'llid': 72, 'length': 1006, 'length_unit': 1006, 'waterway:llid': 1003, 'unnamed': 74, 'key': 2, 'embankment': 5, 'is_in:county': 1, 'loc_name': 3, 'note:lanes': 5, 'addr:country': 72, 'source_ref': 3, 'mooring': 4, 'boundary': 55, 'admin_level': 43, 'history': 4, 'location': 4, 'substance': 2, 'trail_visibility': 30, 'park_ride': 11, 'checked_exists:date': 4, 'wifi': 2, 'microbrewery': 1, 'suite': 1, 'mtb:scale:imba': 9, 'wood': 1, 'covered': 14, 'internet_access': 2, 'name:ru': 1, 'food': 1, 'outdoor_seating': 1, 'reg_name': 6, 'expressway': 2, 'supervised': 4, 'building:use': 1, 'date': 1, 'media': 1, 'sac_id': 1, 'address': 1, 'project': 6, 'historic': 7, 'Geolocation': 1, 'measurements': 1, 'classification': 1, 'artist_last_name': 1, 'artist_first_name': 1, 'floating': 5, 'zoo': 4, 'capacity': 15, 'name_3': 2, 'tiger:name_direction_suffix_3': 2, 'network': 2, 'number': 1, 'military': 3, 'addr:housename': 13, 'access:conditional': 1, 'incline': 15, 'cycleway:right': 6, 'brand': 10, 'fuel:e10': 1, 'fuel:octane_91': 2, 'junction': 11, 'trees': 1, 'ski': 3, 'cutting': 1, 'snowmobile': 3, 'hiking': 7, 'atm': 3, 'drive_through': 5, 'frequency': 29, 'railway:track_class': 3, 'url': 2, 'tidal': 1, 'toll': 6, 'conveying': 2, 'short_name': 3, 'seamark:type': 7, 'seamark:information': 1, 'seamark:small_craft_facility:category': 1, 'mtb': 3, 'motorcycle': 7, 'fence_type': 5, 'wires': 15, 'office': 8, 'contact:phone': 2, 'piste:type': 13, 'piste:difficulty': 12, 'building:part': 19, 'mtb:type': 1, 'gladed': 4, 'piste:grooming': 9, 'designation': 5, 'store_ref': 1, 'tower:type': 1, 'public_transport': 6, 'survey': 1, 'piste:abandoned': 1, 'canvec:CODE': 1, 'seasonal': 1, 'addr:province': 2, 'MTFCC': 5, 'RTTYP': 4, 'STATEFP': 5, 'COUNTYFP': 5, 'LINEARID': 5, 'smoothness': 6, 'bicycle_road': 1, 'oneway:bicycle': 1, 'official_name': 1, 'lanes:hov': 1, 'bus:lanes:conditional': 1, 'hov:lanes:conditional': 1, 'lanes:hov:conditional': 1, 'motor_vehicle:lanes:conditional': 1, 'height': 21, 'motorcycle:lanes': 8, 'hov:lanes:forward': 1, 'name:en': 1, 'FULLNAME': 1, 'piste:name': 2, 'line': 4, 'bus:forward': 3, 'bus:backward': 3, 'bus:lanes:forward': 6, 'bus:lanes:backward': 6, 'access:lanes:forward': 3, 'access:lanes:backward': 4, 'maintained': 1, 'start_date': 4, 'source:addr': 3, 'elevation': 3, 'shape_len': 4, 'shape_area': 4, 'GNIS_Name': 1, 'dogs': 2, 'source:path': 8, 'self_service': 1, 'addr:unit': 40, 'abandoned:highway': 2, 'proposed': 4, 'fut_ref': 1, 'smoking': 3, 'delivery': 1, 'takeaway': 2, 'fuel:diesel': 1, 'social_facility': 3, 'social_facility:for': 3, 'addr:interpolation': 420, 'building:colour': 6, 'golf': 49, '_ZIPL_': 1, '_ZIPR_': 1, '_FEDIRP_': 1, '_FEDIRS_': 1, '_FENAME_': 1, '_FETYPE_': 1, '_FRADDL_': 1, '_FRADDR_': 1, '_TOADDL_': 1, '_TOADDR_': 1, 'vehicle': 2, 'narrow': 1, 'was:amenity': 1, 'train': 1, 'shelter': 2, 'addr:county': 1, 'generator:source': 2, 'seamark:name': 1, 'seamark:calling-in_point:channel': 1, 'seamark:calling-in_point:traffic_flow': 1, 'roof:shape': 5, 'roof:colour': 5, 'roof:height': 2, 'roof:material': 6, 'building:material': 10, 'min_height': 1, 'railway:preserved': 1, 'organic': 1, 'OWNER': 1, 'STATUS': 1, 'SiteNbr': 1, 'TrailID': 1, 'MgmtNotes': 1, 'Functional': 1, 'Management': 1, 'Shape_STLe': 1, 'Shape_ST_1': 1, 'Shape_ST_2': 1, 'SurfaceTyp': 1, 'SysChangeD': 1, 'SysChangeU': 1, 'TrailMaint': 1, 'mtb:scale': 5, 'mtb:scale:uphill': 2, 'addr:full': 1, 'capacity:disabled': 3, 'gate': 1, 'FCLASS': 1, 'LENGTH': 1, 'MUNI_L': 1, 'MUNI_R': 1, 'PREFIX': 1, 'RUCODE': 1, 'CRAB_ID': 1, 'FR_ELEV': 1, 'TO_ELEV': 1, 'CRAB_BMP': 1, 'CRAB_EMP': 1, 'ADDR_TYPE': 1, 'CI_LOG_ID': 1, 'FR_ADDR_L': 1, 'FR_ADDR_R': 1, 'FULL_NAME': 1, 'HIERARCHY': 1, 'RD_LOG_ID': 1, 'TO_ADDR_L': 1, 'TO_ADDR_R': 1, 'SEGMENT_ID': 1, 'STREETNAME': 1, 'STREETTYPE': 1, 'ZIP_CODE_L': 1, 'ZIP_CODE_R': 1, 'tiger:name_type_3': 1, 'FCode': 3, 'FDate': 3, 'FTYPE': 3, 'FlowDir': 3, 'enabled': 3, 'LengthKM': 3, 'ReachCode': 3, 'Permanent_': 3, 'Resolution': 3, 'WBArea_Per': 1, 'opening_date': 1, 'parking:lane:left': 1, 'parking:condition:left': 1, 'comment': 2, 'intermittent': 4, 'noname': 3, 'sidewalk:right:sloped_curb:end': 1, 'old_railway_operator': 2, 'name:zh': 2, 'shelter_type': 2, 'planned': 1, 'source:geometry': 1, 'cycleway:left': 5, 'centre_turn_lane': 7, 'contact:website': 3, 'telecom': 1, 'disused': 1, 'leaf_type': 1, 'overtaking': 2, 'change:lanes:forward': 2, 'change:lanes:backward': 2, 'car_wash': 1, 'craft': 2, 'construction': 1, 'bridge:structure': 1, 'seamark:bridge:clearance_height': 1, 'attraction': 1, 'healthcare:specialty': 1, 'viaduct': 1, 'meadow': 1, 'alt_name_1': 1, 'landcover': 5, 'residential': 2, 'gnis:feature_type': 2, 'handrail': 2, 'ele:fixme': 1, 'waterway_1': 1, 'ferry': 2, 'source:id': 1, 'tram': 1, 'bench': 1, 'gtfs:stop_id': 1, 'gtfs:dataset_id': 1, 'trolley_wire': 2, 'destination:ref:to': 1, 'name:left': 1, 'name:right': 1, 'hov:lanes:backward': 1, 'motor_vehicle:lanes:backward': 1, 'parking:lane:both': 2, 'traffic_calming': 1, 'inner': 9, 'area:highway': 1, 'kerb': 1, 'tactile_paving': 2, 'information': 1, 'crossing_1': 1, 'drinking_water': 1, 'toilets:disposal': 1, 'fax': 1, 'informal': 1, 'maxheight:lanes': 1, 'golf_cart': 4, 'faa': 1, 'rooftop': 1, 'side': 2, 'st_id': 2, 'streets_pk': 2, 'memorial': 1, 'quality': 3, 'shade': 3, 'building:min_level': 3}, 'member': {}, 'relation': {}, 'osm': {}}\n"
     ]
    }
   ],
   "source": [
    "def tag_values(f):\n",
    "    tags = {}\n",
    "    for ev, elem in xET.iterparse(f):\n",
    "        tag = elem.tag\n",
    "        if tag not in tags.keys():\n",
    "            # Create empty set for tag types\n",
    "            tags[tag] = {}\n",
    "        if elem.tag == 'way' or elem.tag == 'node': # Specify parent tag to filter nested tag list\n",
    "            for t in elem.iter('tag'):\n",
    "            #Iterate through tag types, adding new ones to the set for the parent tag\n",
    "                if t.attrib['k'] in tags[tag].keys():\n",
    "                    tags[tag][t.attrib['k']] = tags[tag][t.attrib['k']] + 1\n",
    "                else:\n",
    "                    tags[tag][t.attrib['k']] = 1\n",
    "    return tags\n",
    "\n",
    "\n",
    "def get_tag_values():\n",
    "    tags = tag_values(SAMPLE_NAME)\n",
    "    print(tags)\n",
    "    \n",
    "    \n",
    "#May take in excess of several minutes to run due to the size of the data file\n",
    "get_tag_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the fields that occur most in nodes and ways gives us a good indication of what data occurs frequently in the XML and would be worth investigating for cleanup and parsing into JSON/CSV. Between in the way data, there are 6193 total occurrances of addr:street.\n",
    "\n",
    "The data also contains high numbers of other address elements such as ```:city```, ```:housenumber```, and ```:postcode```. Other fields, such at the ```tiger:``` fields appear interesting as they look like they contain segments of addresses and directions that could be used for uniformity when cleaning the data, but we'll need to look at the data itself to be sure. This is data imported from the US Census Beaureu, but has been heavily edited by users. For the purposes of this audit and import, we will stick to the standard address fields.\n",
    "\n",
    ">Reference: http://wiki.openstreetmap.org/wiki/TIGER\n",
    "\n",
    "Additionally, looking at specific attributes under the ```node``` tag can give us an indication of the type of building or business we're looking at, such as there being 274 denoted shops, or 10907 denoted buildings. ```way``` contains 89 denoted shops, so this will be worth investigating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Shop and Building Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shop': {'outdoor': 2, 'second_hand': 2, 'car_repair': 9, 'craft': 1, 'mall': 2, 'funeral_directors': 1, 'tyres': 1, 'clothes': 2, 'tanning': 1, 'convenience': 12, 'hardware': 2, 'doityourself': 1, 'car': 9, 'supermarket': 7, 'chemist': 1, 'department_store': 2, 'beauty': 3, 'furniture': 2, 'greengrocer': 1, 'scuba_diving': 1, 'pet': 1, 'fabric': 1, 'sports': 1, 'vacant': 2, 'hairdresser': 3, 'deli': 1, 'yes': 3, 'butcher': 1, 'crafts': 1, 'variety_store': 2, 'hobby': 1, 'car_parts': 2, 'gift': 1, 'garden_centre': 2, 'stationery': 2, 'dry_cleaning': 1, 'ticket': 1, 'no': 1}, 'building': {'university': 7, 'yes': 9498, 'commercial': 61, 'school': 37, 'residential': 369, 'house': 610, 'apartments': 92, 'industrial': 13, 'hangar': 5, 'roof': 27, 'office': 4, 'retail': 29, 'dormitory': 2, 'service': 1, 'bunker': 1, 'terrace': 14, 'warehouse': 5, 'detached': 10, 'garages': 7, 'college': 2, 'supermarket': 1, 'public': 4, 'floating_home': 1, 'shed': 16, 'garage': 33, 'church': 6, 'storage': 1, 'static_caravan': 3, 'construction': 1, 'barn': 1, 'greenhouse': 3, 'data_center': 1, 'hospital': 1, 'mobile_home': 26, 'kiosk': 1, 'cabin': 1, 'hut': 1, 'carport': 10, 'farm_auxiliary': 1, 'no': 1}}\n"
     ]
    }
   ],
   "source": [
    "ATTRIBUTES = ['shop', 'building']\n",
    "\n",
    "def count_types(f):\n",
    "    types = {}\n",
    "    \n",
    "    for a in ATTRIBUTES:\n",
    "        types[a] = {}\n",
    "        \n",
    "        for ev, elem in xET.iterparse(f):\n",
    "            tag = elem.tag\n",
    "            if elem.tag == 'way':\n",
    "                for t in elem.iter('tag'):\n",
    "                    if t.attrib['k'] == a:\n",
    "                        if t.attrib['v'] not in types[a].keys():\n",
    "                            types[a][t.attrib['v']] = 1\n",
    "                        else:\n",
    "                            types[a][t.attrib['v']] = types[a][t.attrib['v']] + 1\n",
    "            \n",
    "    return types\n",
    "\n",
    "def get_type_count():\n",
    "    types = count_types(SAMPLE_NAME)\n",
    "    print(types)\n",
    "    \n",
    "#May take in excess of several minutes to run due to the size of the data file\n",
    "get_type_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script gives us a really detailed breakdown of the types of shops and building that are in the sample data. Judging by how random these tags are, it's apparent these keywords are neglected by a lot of users, and in many cases aren't consistent when trying to descibe similar businesses or buildings (such as supermarket and greengrocer). \n",
    "\n",
    "Additionally, it seems like most users use the building attribute as a boolean yes/no value, while others use it as a description of the type of building. this type of inconsistent record keeping could really throw off understandings of how this field should be tracked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the data included in this list of fields is descriptive information about ways themselves, but does not include other metadata such as users, and information about each entry. Digging further into the shape of this data will also allow us to figure out the metadata that's available to go with this map data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'way20': {'changeset': '90945',\n",
      "           'id': '4736364',\n",
      "           'nodes': {'30176530',\n",
      "                     '30178412',\n",
      "                     '30183676',\n",
      "                     '30196045',\n",
      "                     '30197347'},\n",
      "           'tags': {'created_by': 'JOSM',\n",
      "                    'from_address_left': '198',\n",
      "                    'from_address_right': '199',\n",
      "                    'highway': 'residential',\n",
      "                    'name': 'Gretchen Way',\n",
      "                    'name_base': 'Gretchen',\n",
      "                    'name_type': 'Way',\n",
      "                    'reviewed': 'no',\n",
      "                    'separated': 'no',\n",
      "                    'source': 'tiger_import_20070610',\n",
      "                    'tiger:cfcc': 'A41',\n",
      "                    'tiger:tlid': '152178976',\n",
      "                    'to_address_left': '170',\n",
      "                    'to_address_right': '171',\n",
      "                    'zip_left': '98250',\n",
      "                    'zip_right': '98250'},\n",
      "           'timestamp': '2007-06-11T11:01:27Z',\n",
      "           'uid': '8609',\n",
      "           'user': 'ewedistrict',\n",
      "           'version': '1'},\n",
      " 'way21': {'changeset': '34784129',\n",
      "           'id': '4736566',\n",
      "           'nodes': {'30170938',\n",
      "                     '30178442',\n",
      "                     '30179370',\n",
      "                     '30187873',\n",
      "                     '3789606320',\n",
      "                     '3789606321',\n",
      "                     '3789606322'},\n",
      "           'tags': {'access': 'destination',\n",
      "                    'from_address_left': '153',\n",
      "                    'highway': 'residential',\n",
      "                    'name': 'Tennis Avenue',\n",
      "                    'name_base': 'Tennis',\n",
      "                    'name_type': 'Ave',\n",
      "                    'reviewed': 'no',\n",
      "                    'separated': 'no',\n",
      "                    'source': 'tiger_import_20070610',\n",
      "                    'tiger:cfcc': 'A41',\n",
      "                    'to_address_left': '259',\n",
      "                    'zip_left': '98261'},\n",
      "           'timestamp': '2015-10-21T17:46:00Z',\n",
      "           'uid': '3305376',\n",
      "           'user': 'Madrona',\n",
      "           'version': '4'},\n",
      " 'way22': {'changeset': '34849927',\n",
      "           'id': '4736790',\n",
      "           'nodes': {'30185256',\n",
      "                     '30187679',\n",
      "                     '3801391454',\n",
      "                     '3801391562',\n",
      "                     '3801391568',\n",
      "                     '3801391571',\n",
      "                     '3801391573',\n",
      "                     '3801391580'},\n",
      "           'tags': {'from_address_left': '4298',\n",
      "                    'from_address_right': '4299',\n",
      "                    'highway': 'secondary',\n",
      "                    'name': 'Center Road',\n",
      "                    'name_base': 'Center',\n",
      "                    'name_type': 'Rd',\n",
      "                    'reviewed': 'no',\n",
      "                    'separated': 'no',\n",
      "                    'source': 'tiger_import_20070610',\n",
      "                    'tiger:cfcc': 'A41',\n",
      "                    'to_address_left': '4100',\n",
      "                    'to_address_right': '4101',\n",
      "                    'zip_left': '98261',\n",
      "                    'zip_right': '98261'},\n",
      "           'timestamp': '2015-10-24T21:42:59Z',\n",
      "           'uid': '3305376',\n",
      "           'user': 'Madrona',\n",
      "           'version': '3'},\n",
      " 'way23': {'changeset': '34848430',\n",
      "           'id': '4737026',\n",
      "           'nodes': {'30169227',\n",
      "                     '30176768',\n",
      "                     '30181264',\n",
      "                     '30184139',\n",
      "                     '30187091',\n",
      "                     '30188611',\n",
      "                     '30191194',\n",
      "                     '30197583',\n",
      "                     '3789110464',\n",
      "                     '3789110465',\n",
      "                     '3789110466',\n",
      "                     '3789110467',\n",
      "                     '3789110468',\n",
      "                     '3789110469',\n",
      "                     '3789110470',\n",
      "                     '3789110471',\n",
      "                     '3789110472',\n",
      "                     '3789110473',\n",
      "                     '3789110474',\n",
      "                     '3789110475',\n",
      "                     '3789110476',\n",
      "                     '3789110483',\n",
      "                     '3789110484',\n",
      "                     '3789110485',\n",
      "                     '3789110486',\n",
      "                     '3801272337',\n",
      "                     '3801272345',\n",
      "                     '3801274362',\n",
      "                     '3801274422',\n",
      "                     '3801274424',\n",
      "                     '3801274437',\n",
      "                     '3801274452',\n",
      "                     '3801274455',\n",
      "                     '3801274463',\n",
      "                     '3801274480',\n",
      "                     '3801274485',\n",
      "                     '3801274518'},\n",
      "           'tags': {'from_address_left': '26',\n",
      "                    'from_address_right': '423',\n",
      "                    'highway': 'residential',\n",
      "                    'name': 'Hooterville Lane',\n",
      "                    'name_base': 'Hooterville',\n",
      "                    'name_type': 'Ln',\n",
      "                    'reviewed': 'no',\n",
      "                    'separated': 'no',\n",
      "                    'source': 'tiger_import_20070610',\n",
      "                    'tiger:cfcc': 'A41',\n",
      "                    'to_address_left': '212',\n",
      "                    'to_address_right': '665',\n",
      "                    'zip_left': '98261',\n",
      "                    'zip_right': '98261'},\n",
      "           'timestamp': '2015-10-24T20:18:04Z',\n",
      "           'uid': '3305376',\n",
      "           'user': 'Madrona',\n",
      "           'version': '4'}}\n"
     ]
    }
   ],
   "source": [
    "# Metadata\n",
    "METADATA = [ 'version', 'changeset', 'timestamp', 'user', 'uid', 'id']\n",
    "\n",
    "# Street Types in Addresses\n",
    "st_types = defaultdict(set)\n",
    "\n",
    "\n",
    "def preview_data(tag):\n",
    "    i = 0\n",
    "    s = 20\n",
    "    n = s + 3  # Number of tags to preview\n",
    "    pv = {}\n",
    "    for event, elem in xET.iterparse(SAMPLE_NAME, events=('start',)):\n",
    "        if elem.tag == tag:\n",
    "            i += 1\n",
    "            if i >= s and i<= n:\n",
    "                pv[tag + str(i)] = {}                \n",
    "                for a in METADATA:\n",
    "                    if elem.attrib[a] not in pv[tag + str(i)].keys():\n",
    "                        pv[tag + str(i)][a] = elem.attrib[a]\n",
    "                        \n",
    "                pv = get_tag_list(pv, elem, tag, i)\n",
    "                pv = get_node_list(pv, elem, tag, i)  \n",
    "                \n",
    "                if i == n:\n",
    "                    return pv\n",
    "                \n",
    "                \n",
    "def get_tag_list(pv, elem, tag, i):\n",
    "    pv[tag + str(i)]['tags'] = {}\n",
    "    for t in elem.iter('tag'):\n",
    "        #Iterate through tags, adding new ones to the set for the parent tag\n",
    "        if t.attrib['k'] not in pv[tag + str(i)]['tags'].keys():\n",
    "            pv[tag + str(i)]['tags'][t.attrib['k']] = t.get('v')\n",
    "    return pv\n",
    "\n",
    "\n",
    "def get_node_list(pv, elem, tag, i):\n",
    "    pv[tag + str(i)]['nodes'] = set()\n",
    "    for nd in elem.iter('nd'):\n",
    "    #Iterate through nodes, adding new ones to the set for the parent tag\n",
    "        pv[tag + str(i)]['nodes'].add(nd.attrib['ref'])   \n",
    "    return pv\n",
    "\n",
    "pprint.pprint(preview_data('way'))       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here was can see the metadata is now included, and we have a small sample of the shape of the ```way``` data. This also shows us that some way entires don't have all the information about an address, so when we're parsing and cleaning data, this is something we'll need to consider. \n",
    "\n",
    "To get an idea of how many people have been involved in the creation of this sample data, we'll take a quick peek at some aggregated user information. We can also see that the ```name``` attribute contains street address information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Users:  1558\n",
      "{'Torsang', 'keepright! ler', 'Syrn', 'KarlaQat', 'godfd379', 'rando67', 'jinalfoflia', 'dethme0w', 'kona314', 'japerry', 'dankgnu', 'emmdoerr', 'AlexRu', 'Natfoot', 'Whitt-E', 'JJMAR', 'sea duck', 'Dilys', 'jBeata', 'MarcEscape', 'kasims', 'djholman', 'Sappe', 'SydneyCarpenter', 'buckey206', 'sebastic', 'Bigcamper', 'Dampee', 'bal_agates', 'Contre', 'neuhausr', 'bwhill', 'MisterOblivious', 'Darryl Karleen', 'cullanp', 'Wrenling', 'Armin Zimmermann', 'Brian Gant', 'schann16', 'Maxim Velichko', 'aaronracicot', 'MikeGost', 'will simms', 'CartographerC', 'nstarksen', 'Paul Buxton', 'Luciola', 'StephenMangum', 'Jyoti Naik', 'Brian2112', 'Chris Lawrence', 'jamesholio', 'Ropino', 'euxneks', 'MappingJunkie', 'DennisL', 'Paul McCombs_Import', 'IanRoskelley', 'nbolten_import', 'WBSKI', 'cdbreiland', 'jacalata', 'FailMeh', 'The Rev', 'Tagalongs', 'DCD1', 'Constable', 'dkav', 'charles92', 'csytsma', 'PhilNi', 'CalliBrown', 'AlexZolotarev', 'Komяpa', 'petersfreeman', 'Chris Rogers', 'rcorbett', 'Alecs01', 'ruph', 'Jason Chu', 'ChrisZontine', 'firstfleet', 'Jess Hamilton', 'mjn', 'Alchemist', 'Korikin', 'bobschriver', 'Jesse Baker', 'chris@zontine', 'jhudsonwa', 'Rhizomes', 'samuelestabrook', 'MrArfArf', 'sarahjanepratt', 'Lazarevsk', 'James Donohoe', 'gisgal', 'Grant Colasurdo', 'Pixtur', 'Karsten Natebus', 'alester_imports', 'AtonX', 'Craig Williams', 'z-dude', 'djphilero', 'Goldenowl12', 'Larry Lowry', 'Andre68', 'Dano47', 'wilsoms', 'John Peterson', 'ThorstenNeumann', 'bdferris', 'TwistedSage', 'therobison', 'Anthony Bloesch', 'Vashonista', 'Tyrel', 'gnicol', 'rtsfred3', 'JALIX', 'Cyrusis', 'sobpup', 'GoWestTravel', 'MappedReduced', 'RichManSCTV0', 'homeslice60148', 'Michi', 'TIGERcnl', 'ZymoDog', 'MilaZ', 'alisonjoymiller', 'Omnific', 'pxYyFQZkbA', 'mwra-uk', 'jduggan', 'derFred', 'JesperFj', 'bkuennen', 'hefee', 'scai', 'nickpeihl', 'ramyaragupathy', 'fmarier', 'vitaliypaul', 'Sudobangbang', 'TheHammer', 'Ijordan', 'baditaflorin', 'thomasperre', 'chalen', 'emem', 'greengo', 'MuzikMachine', 'Brad K', 'Maskulinum', 'ToffeHoff', 'beweta', 'Phil Shipley', 'Markcharcas', 'ingalls', 'joncp', 'theonlytruth', '1248', 'SpeedLineSupport', 'questionsandtea', 'saikabhi', 'Tronikon', 'iandees', 'dirtbikejunkie', 'wallclimber21', 'chuybregts', 'thevirginian', 'LucasCotner', 'svance92', 'BabarT', 'Freddy Johnson', 'BassoonCat', 'StaticLimit', 'Walter Wittel', 'axodys', 'narwhalofages', 'Nazar Paslavsky', 'Zonn', 'Maksym Bondarenko', 'hofoen', 'EPIT22', 'MadMax3110', 'Jonah Dunham', 'steverumizen', 'Radnor', 'Nate57', 'selfnature', 'WTØF', 'Amoebabadass', 'DevonF', 'grosny_import', 'Ashley Rose', 'xyzl', 'kpow', 'Bill Blake', 'Skybunny', 'arnaud_mapali', 'aug_aug', 'andrewpmk', 'Aaron Lidman', 'Djrachelamaya', 'bedle', 'BlueMagnetInteractive', 'asdf1234', 'cowdog', 'dycl', 'rvorvor', 'liltrainerboi', 'jwass', 'bdp', 'Sitter', 'nammala', 'tonbach', 'coughie', 'nmixter', 'Squig', 'sansell', 'WallyB', 'Blair T', 'anst', 'CongoMan', 'PHerison', 'brightfeather', 'PlacedDu', 'CD5MESSI', 'bricker', 'JennyLynne', 'AKJo', 'CaptBeth', 'awg1010', 'Derek Benigno', 'kugelfiesch', 'dbeale592', 'BB_', 'Rob Lanphier', 'Binary Alchemy', 'Jack the Ripper', 'kriscarle', 'mappyface', 'MapCrasher', 'Djoto', 'Ivan Komarov', 'canicol', 'dforsi', 'naturevictoria', 'Ars_77_new', 'ArataD', 'Thomas Pohle', 'zlh', 'Stephen214', 'Francisco Pickens', 'allier', 'Andrew Ressa', 'Hvalur', 'John Hetherington XI', 'rebcabin', 'Data411', 'pluton_od', 'Abu Mapper al-Baghdadi', 'the1unoriginal', 'alexkaz', 'scytacki', 'MappingSnoqualmie', 'Rayvenhaus', 'TomHynes', 'NickBolten', 'silversmithw', 'brettcannon', 'jamiestarke', 'Peda', 'PNW_adam', 'vedichymn', 'Heather Racicot', 'emorye', 'MGH', 'mattvalerio', 'techlady', 'olywiki', 'penguinpetter', 'palimpadum', 'keving12', 'WecWec', 'NJonquil', 'user_5359', 'tobint', 'gormur', 'RainbowDashDC', 'DerekBentley', 'StellanL', 'sladen', 'Griffin44', 'Dirtyspork', 'sandhill', 'maxolasersquad', 'Cdogg52', 'Eric Fischer', 'makaeser', 'n0rd', 'CaiB', 'Homsar123', 'gerv', 'Sherweld', 'calfarome', 'gaminup', 'har777', 'Hjart', 'NE3', 'juuroku', 'Dave Shaw', 'snce5', 'dieterdreist', 'magicalpixie4u', 'cglazier', 'ChiefKC', 'Chris Fadden', 'NE2', 'Jonathan Nelson', 'Indent', 'beej71', 'Sunny', 'fx99', 'lukeandrew', 'LaceyGIS', 'compdude', 'Meterpig', 'kdruin', 'a6y', 'Belden Lyman', 'domoritz', 'davidhelpsopen', 'Jesse Phillips', 'freeups', 'Matthew Kennedy', 'Tim BL', 'Buschebabau', 'Holly1', 'freelock', 'imgary2u', 'Kate Diaz', 'Kent Diamond', 'stangoodman', 'Jorge Restrepo', 'A John Alger', 'Husky59', 'Ranek', 'HokiePilot', 'Seth McKinney', 'Question Mark', 'Md Alamgir', 'Dan_S', 'jwjchap', 'Bookbinder', 'jwalseth', 'Austin Henry', 'B-rock', 'FTA', 'knu5010', 'merryxmas', 'jsthoreson', 'Whelan Photography', 'Mannhammer', 'AD5', 'davidphorne', 'chricycle', 'jpweber', 'TreeTree', 'Husqvarnic', '11Runner', 'TheOutpost', 'AndyMap', 'ChuggyChug', 'lanefelker_import', 'GlennFleishman', 'TEG24601', 'fliesenleger63', 'avs5221', 'geoglrb', 'Kurly', 'Oswaldo Ribas', '_mapsam', 'fredjunod', 'rodeochick', 'Ryan Bahm', 'GIS_Katie', 'sharifau', 'cedarhillgoods', 'MHnkl', 'samely', 'CraftyCBG', 'ewedistrict', 'jedr', 'Doug Seven', 'elbatrop', 'Palolo', 'seamxr', 'joshparnham', \"Aaron's Bicycle Repair\", 'COBAddressing', 'Jason Robar', 'the Sandinator', 'Skippern', 'jknewl', 'Steve Hollasch', 'dcs', 'silent voyager', 'Dr Kludge', 'eulochon', 'Heron4519', 'btonkin', 'dchiles', 'jamesxxd', 'RussNelson', 'GNW312', 'bitslab', 'mojodna', 'jc86035', 'lovethetrails', 'GoMapDemo', 'Sonyr', 'mdrouhard', 'thinkling', 'bryceco', 'Stephanie Landa', 'izekielrage', 'transplan331', 'market72', 'jonmccon', 'Czmartin', 'D Murray', 'dmgroom', 'goldenfly5', 'Robert Whittaker', 'Bman', 'LeTopographeFou', 'Kena_Lynn', 'Rick Oakley', 'dpp', 'gfish', 'extesser', 'brianvm', 'Thienake Saisung', 'Tim Litwiller', 'Glassman_Import', 'Sat', 'Steve', 'Olivier Calle', 'germin8', 'defiantShaun', '박제헌', 'Fullfill', 'Daniel Szwalkiewicz', 'wegavision', 'celesaku', '--IKS--', 'pseudotsuga', 'jollywollup', 'RichRico', 'Xzarno', 'Kontrolled', 'gps-newcomer', 'mash84121', 'metrodriver', 'mattropolis_bulk', 'Cato_d_Ae', 'Aury88', 'TraceyT', 'bsidhom', 'MojaveNC', 'LucasLarson', 'ryandrake', 'Manu1400', 'Blazie', 'Robert Gash', 'andygol', 'amillar', 'pkm', 'dkwolf', 'searker', 'Tonelock', 'skoocoom', 'ByeFelica', 'SounderBruce', 'Peaceray', 'kartler175', 'Young Outdoors Man', 'JFK73', 'TheOtherOne', 'animeigo', 'MatthewAnderson707', 'Michael Rosen', 'keenfactor', 'I like traffic lights', 'LawrenHB', 'frasergr', 'digdug', 'Autarch', 'Michael Fidler', 'wvdp', 'kgallant', 'Evelyn Desouzaa', 'rcr', 'Schandlers', 'Brian Reavis', 'Charlie Loyd', 'SuzanneH', 'jvahsholtz', 'Derek Westcott', 'aarp65', 'RobTheGob', 'dannykath', 'AidanOsb', 'christianstorer', 'Greg_Rose', 'Toblakai', 'ellieoftheworld', 'stefohnee', 'Typhlosion157', 'Wim L', 'li3n3', 'bstrub', 'rolandg', 'Walter Moar', 'Jay P', 'osmostrix0_import', 'briansquared', 'ghph', 'Sven L', 'Richard', 'malcolmh', 'devinpray', 'romanpohorecki', 'StreetWalker64', 'Spanholz', 'Yaron', 'Liz KOS', 'Dean Crawford', 'piligab', 'ObviousDWest', 'phroa', 'DoubleD', 'dellorto', 'katiethomson', 'txrxio', 'kaboomie', 'jjimmyb', 'Igor85', 'kay_D', 'Jamie Popkin', 'mackerski', 'mll1013', 'SterlingGIS', 'Noenflux', 'mvexel', 'JDong', 'lorandr_telenav', 'BrendenM_NPS', 'lonvia', 'brentosj', 'gpoo', 'abschiff', 'jacobbraeutigam', 'Xaph', 'Joe_G', \"Séamus O'Connor\", 'GITNE', 'uxrs75', 'mtingley84', 'apollolm', 'Justin R', 'Yehoshua', 'DougM', 'gtheil02', 'ToeBee', 'geochrome', 'ffrogers', 'skyboysea', 'MTB Guy', 'Brandon Reavis', 'Dami_Tn', 'Paul Ramsey', 'nicolas17', 'Deanna Earley', 'fmot', 'ayleph', 'bbessembinders', 'Samuel Emmett Bray', 'ghostrecondab', 'pyram', 'John Huebbe', 'geoffengland', 'jovin01', 'Debbie Bull', 'doug_sfba', 'marksvend', 'HMaltbie04', 'inah_telenav', 'Vatick', 'Tim Sneath', 'GarThor', 'ajcastro', 'Dragonpaw', 'Subba_K', 'Bellbellbell', 'J_Plattner', 'ruthmaben', 'trowhite', 'nancyallb', 'xcboy', 'CGHRocks', 'Rps333', 'hamling', 'vit', 'goldfndr', 'MillCreeker', 'stephbu', 'andyw248', 'ronicat', 'schnelli', 'SteveMills', 'KashG', 'LarsBG', 'StrangerVP', 'ShoopM', 'samsnickles', 'BC_Trailbuilder', 'Foundatron', 'Marcussacapuces91', 'VMeyer', 'Micah3', 'AndrewBuck', 'NormLWinchester', 'Steven Deeds', 'tmckenzie', 'Peter14', 'mr_martacus', 'coyotebush', 'Billygoatc', 'Vishal B Sathyamurthy', 'martin0203', 'DENelson83', 'bbessembinders_import', 'pathym', 'wldfyre', 'flockfinder', 'Dexorcist', 'xybot', 'warneke7', 'links4jeeps', 'MintCondition', 'vagabond', 'lukobe', 'eptin', 'Andrey Kazantsev', 'JSummerson', 'Anton Lavrov', 'Nick_T', 'waterocean', 'moosejaw', 'reilly3000', 'KShawcroft', 'marksalot', 'ph_pac_nw', 'dcp', 'GregRetro', 'mota37', 'Dinglehoser', 'Apo42', 'w9nwrwi', 'suzyb34', 'Henry H', 'STBrenden', 'ac7ss', 'gt5', 'schmerzbereiter', 'noexit', 'sfreelan', 'Easky30', 'lissom3d', 'GeekAthair', 'elsaxo', 'jemc', 'Andrew*Robinson', 'Divya Mahajan', 'OSMF Redaction Account', 'OrcaDan', 'Jackyy', 'freeExec', 'davidtheduckfan', 'georges_telenav', 'VolticRay', 'roptat', 'Brooke \"Data Slayer\" Mahnken', 'BC Hiker', 'Creando', 'DCJoeS', 'sergal', 'jfcvsrxb', 'Speed Bird', 'RoadGeek_MD99', 'dlrag', 'Toliman', 'hno2', 'jacktackular', 'regorman', 'EmanuelKant', 'ThoranX', 'MTB54', 'cjc343', 'alexz3', 'Theodin', '_Dima', 'Cheshiredon', 'oranv', 'Island Human', 'effektz', 'Gurunandan Krishnan', 'Misha Mokhov', 'Langry', 'dalek2point3', 'imoo_maps', 'soonerjcb', 'Jose De Santos', 'osm-sputnik', 'awisher', 'ulrichm', 'RD1', 'bbyrd', 'Alex-7', 'drixomanbeta', 'MuscledCarrot', 'Kenneth Pardue', 'rw__', 'Geogast', 'mkochubey', 'jeffrando', 'TheDutchMan13', 'Extramiler', 'highbuilder', 'PeterAv', 'mgaffney', 'MikeWA', 'RossAlGhul', 'PlaneMad', 'MasiMaster', 'becw', 'WernerP', 'Nimbalo', 'DJ13Chef', 'Omar Bonilla', 'widesays', 'Jacob Marzynski', 'ShaynaBarnes', 'JLZIMMERMANN', 'TracyQi', 'stephspizza', 'Dickson Dabell', 'wheelmap_visitor', 'jakeroot', 'AndrewSnow', 'JohnEgencia', 'mdk', 'FigBug', 'Kwang69', 'eriosw', 'Brian@Brea', 'turiabbott', 'mbela', 'Joggele', 'mango123', 'abel801', 'Семён Семёнов', 'rellingwood', 'rainnw', 'BD25', 'Stella Lee', 'briandela', 'ebwolf', 'sctrojan79', 'Harry Wood', 'Wayne VanWeerthuizen', 'Grauer Elefant', 'InfiNorth', 'Adam Martin', 'Skylos', 'mikelsch', 'kmt023', 'curiouscomp', 'Ani_Cosmos', 'UserXYZ', 'Jonah Adkins', 'gilly3', 'Jwtc', 'Mert Emin Kalender', 'David Lowe 06', 'skquinn', 'Zirnch', 'h4ck3rm1k3', 'Brad Meteor', 'steveh1', 'Andrew colin', 'Jesús Gómez', 'TorbenE', 'monkeystyping', 'pecknoldrules', 'ajashton', 'Erikk Machowek', 'pdelon', 'jgrimes900', 'AdamGlass', 'pslarkin', 'Theodrel', 'NebbyPal', 'Tomasz11', '123maps', 'elirnm', 'DavidGeeraerts', 'Jacgoat', 'Blue Nacho', 'skwash', 'Claumires', 'gutrun', 'dasterdly', 'TimuSumisu', 'rickmastfan67', 'timgoddard', 'BlackTy', 'sc694', 'Stalfur', 'fossosm', 'recliner911', 'Squirrel', 'yurasi', 'ianobermiller', 'markuspeloquin', 'KevinRobertson', 'Johnny Mapperseed', 'jeepjockey', 'Iowa Kid', 'yotann', 'xeen', 'scearley', 'RNSWA', 'Leifab', 'Hundehalter', 'Kristymarie42', 'JeffM8', 'LiesLiesLies', 'Nate_Wessel', 'pjna', 'KaiTing Hsu', 'mac99201', 'tbw875', 'Pracheer', 'Another206er', 'SveLil', 'bhazelton', 'Jonathan Viray', 'ConnorB', 'danbjoseph', 'StickyNotePile', 'Dave not here', 'PotatoHead2', 'Mimich6120', 'cckellin', 'bab72', 'stevedave', 'hkleen', 'histat46', 'Buskape', 'Derick Rethans', 'MWV523', 'noliver', 'MikeN', 'Skulldozer', 'Kieran Delach', 'WhealKitty', 'jessebibee', 'Latze', 'CarniLvr79', 'Jordan Ryan Moore', 'grdomzal', 'SK53', 'skagit', 'Glenn29502', 'DNButler', 'NoWayHome', 'MatMoore218', 'climbbike', '25or6to4', 'La Chaussiére', 'Peter Shank', 'bldr-roamer', 'Chuck Taylor', 'jaz908', 'makoshark', 'Cobra', 'kgienger', 'keinseier', 'puzzlr', 'AskCDE', 'Taso', 'SP!KE', 'swatspyder', 'TorCguy', 'BortSimpson', 'yoi-man', 'seattle-buildings', 'Choong', 'Heinz_V', 'Frenchman in Seattle', 'H992B', 'Uffe Hellum', 'seichter', 'Fa7C0N', 'ercjns', 'craigloftus', 'kmacc', 'chef bama', 'Scott Leibrand', 'Stephen Magladry', 'ElliottPlack', 'kc12345', 'SuzanneSy', 'paulmach', 'DMaximus', 'Abbe98', 'Yoshinion', 'Acizza', 'mgoe', 'zephyr', 'barrymapper', 'Bootprint', 'scarecrow93', 'Lucien Ong', 'Dog_cachers', 'Scott Huntmeister', 'monikah', 'pnorman', 'TaskarCenterAtUW', 'srividya_c', 'hobbesvsboyle', 'tjbenator', 'Highspeed1964', 'robbjt', 'patrickjhalf', 'Pnrrth', 'roadgeek99', 'MichaelCollinson', 'MarkC514', 'bruhnstv', 'hooperbloob', 'mstingm', 'atenasfigueroa', 'mueschel', 'spoonyboi', 'Mark Gray', 'Mham27', 'lxbarth', 'Magduf', 'mysweetsam', 'Jaimoe', 'Hai-Etlik', 'jonesydesign', 'nkuipers', 'herrgeigel', 'Tschanz', 'Sundance', 'Vlad', 'GTU GISP', 'codemonkee', 'BlueCanoe', 'oceanearth', 'Markus B', 'wambacher', 'pmakevin', 'sRasmussen', 'Frank Quam III', 'Reclus', 'GMCTurbo', 'mlouns', 'JeffsBees', 'Chasfire', 'PCS', 'Tracy_Mehlin', 'eric22', 'Devin2013', 'PurpleMustang', 'Ceema', 'who_is_john_galt', 'permute', 'Stan Robertson', 'srparish', 'Madrona', 'sakudo', 'Sowa1980', 'DueWestGIS', 'Hartmut Holzgraefe', 'Luis36995', 'atan33', 'jeffcool', 'zehpunktbarron', 'Dion Dock', 'waptug', 'Carson321', 'aleishaj', 'kisaa', 'jtkriese', 'michalis', 'VeloBusDriver', 'BenJpilot', 'nauseouscrow', 'Matt', 'davidearl', 'glnbrn', 'Emmylenny123', 'osm2xp', 'tfooq', 'rohangpatil', 'Steve138', 'Russ', 'John Ewald', 'ediyes', 'SK53_bulk', 'fishercraigj', 'robgeb', 'CaiB_import', 'ltvine', 'Arlas', 'Bob Heddle', 'davidfg4', 'eugene-lisovskiy', 'bkerr', 'pschonmann', 'Steve Brandli', 'Munchabunch', 'Rdouglas1235', 'upendrakarukonda', 'Mined20', 'JimTheFrog', 'GnuUser', 'FTA_dwg', 'mikestew', 'pnorman_mechanical', 'talapus', 'robkline', 'filmfan2206', 'FvGordon', 'VirtualHannes', 'bricelam', 'woody1975', 'RoadHound', 'Brian(J)', 'LostSparky', 'Ryan Zimmerman', 'sir Veyor', 'seattlefyi', 'pyrog', 'CharlesWTA', 'jabula', 'lsetiawan', 'manings', 'seattlescout', 'anansi133', 'llee4PRI', 'Léo Frachet', 'as2814', 'jeepin95', 'CloCkWeRX', 'JazzyJ96', 'Spudboy', 'Hamad0502', 'KTyler', 'stoth', 'TEDitlefsen', 'SteveDorries', 'ring1967', 'Jacob Kukuk', 'Todd Schmalhurst', 'goofyspouse', 'Jesse Langdon', 'Paul McCombs', 'KaleenaSawyer', 'Falcon Darkstar Momot', 'GapSpanner', 'DuncLaw', 'hq712', 'DanJSY', 'partim', 'EugeneWII', 'Karsten of the Island', 'Miguel Ludert', 'meebie2', 'bigalxyz123', 'CapC', 'matie cakes', 'byersbs', 'keithonearth', 'Jón', 'Meliora Cogito', 'Shrekznuts', 'joepavey', 'ejain', 'pete404', 'MariusStrom', 'kh72022', 'Brit541', 'riverflow', 'CdE', 'Johannes Erdfelt', 'Jason Roy', 'Dave Pitman', 'mdp553', 'Trekoid', 'xStreetKingx', 'MikeSea', 'RetiredInNH', 'Savinay', 'mholiv', 'legolin', 'Dave-_-1', 'Tori Wilson', 'claysmalley', 'jfire', 'cougarfan75', 'ajturner', 'Sarah M', 'daansteeman', 'Big Josh', 'oba510', \"Ethan O'Connor\", 'aiwetir', 'Burger', 'tbearman', 'ccobb123', 'Heptazane', 'qwertylool', 'btpreparedresponse', 'Юкатан', 'jhawe', 'isr', 'Chiuaua', 'Lundtoft', 'Brian Bancroft', 'wxwatcher49', 'rminkler', 'ShaneC', 'stuagano', 'DKNOTT', 'gissteve', 'Jaime Cartes', 'stevea', 'BugBuster', 'Olyon', 'David & Christine Schmitt', 'AlaskaDave', 'headwatersolver', 'Mapper666', 'Jrickclark', 'bahnpirat', 'FixerDave', 'davecotter', 'Johnny Carlsen', 'Iain I', 'Mike Spiz', 'dunbar loop', 'clairehhlin', 'Akihiko Lin', 'derredu', 'Fitch', 'TorhamZed', 'Klavierspieler', 'chronomex', 'hunterdt', 'tcao@transloc', 'sookehills', 'bman97', 'Caesura', 'Peter K', 'ladkahaidesi', 'jejb', 'Bryce C Nesbitt', 'TheGoodman', 'Camwow13', 'Ymakur10', 'geohacker', 'Jothirnadh', 'Pfly', 'aott', 'Dave Guarino', 'gimdim', 'David ColbyHutchison', 'discostu37', 'EC95', 'Ritar33', 'swimdb', 'Muzzle', 'sjharper', 'AaronPeterson', 'Federico2', 'David W Robinson', 'Paul Johnson', 'rambudo', 'ThePharmacyst', 'jimbo7771', 'th12teen', 'maddog03', 'kirien', 'OrangeBlue', 'moojieturtle', 'matthieun', 'blueal', 'woodpeck_repair', 'parkranger', 'wilsoncd35', 'Eric Godwin', '>LAND', 'yasobara', 'DanO93', 'p-merritt', 'MandaRose', 'mapmeld', 'TOGA', 'ukuester', 'Nikki G', 'zacharilius', 'Danrh', 'trestrini', 'Taylorjk', 'Michael Mansell', 'Seadue', 'WASurveyor', 'jtreuting', 'Pyro Fox', 'kyle carpenter', 'pingsler', 'HurricaneCoast', 'ClintPells', 'dmgroom_ct', 'SeattleImport', 'joelotz', 'Battlehardened', 'benjaminwil', 'DustyBroom', 'Darkii2505', 'EdHillsman', 'Kazaraque', 'Thyais Meade', 'Tom_Holland', 'MikeyDP', 'SheenaH', 'RangerBob', 'Serguei Okountsev', 'Jim Grisham', 'jjarc136', 'jeffmeyer', 'Rapiddash189', 'christianechandler', 'vonvonvon', 'syershov', 'licyeus', 'Иван Пироговский', 'spinninthewheel', 'slaberee', 'IGTaylor', 'MarioXXX', 'ayushupreti22', 'cohort', 'Jnorberg', 'keppyhouse', 'appellation', 'SomeoneElse_Revert', 'skorasaurus', 'giovino', 'V Savin', 'Pilot Guy', 'n76', 'rduecyg', 'tylerritchie', 'Yeliena', 'Barfly_MB', 'Gregor7', 'jlkredmond', 'Imp_GL', 'photogTM', 'ridixcr', 'Dschulte', 'Gustavo75', '0ly36O', '32x17', 'Mike Shappell', 'braxj', 'salix01', 'Jason Overland', 'funtervention', 'nejones', 'shfishburn', 'Steve00h', 'karitotp', 'BallardMapper2012', 'faronium', 'hd7jsjjd99sdd', 'JeffJ', 'spuckett', 'nw124', 'Berjoh', 'McKayla', 'BCNorwich', 'iPhD', 'arisphinx', 'ray laforme', 'evopix', 'KristenK', 'KSharm', 'TristanA', 'N1029676', 'Martha Applegate', 'Aspadistra at Ashdown Forest', 'Will Thomas', 'JerezzPrime', 'Eyal Ofek', 'Kevo', 'WithinRafael', 'ig6y', 'catrip11', 'woodpeck_fixbot', 'Ryan Peterson', 'Howpper', 'LeslieStrom', 'CoreyBurger', 'mtpmedia', 'CoderDennis', 'Tom Morris', 'bugmenot2', 'ChunKwan88', 'Timothy Smith', 'Madeline Steele', 'srt4', 'Lamoreaux', 'Uncle David', 'jmwiley', 'flek', 'posdata', 'HeZiX', 'alejandrobg', 'tuxayo', 'SKald', 'gforman44', 'choess', 'grhardy3', 'Kla_Ger', 'Bailey in Kitsap', 'adamants', 'SteveC', 'bryantw', 'Helmchen42', 'ArtiePenguin1', 'Sarr_Cat', 'JSeattle', 'GerdP', 'Neurus', 'billatq', 'jonhughes', 'Brittany Port', 'Madongo', 'MarkUpMapper', 'ktula', 'beezwax notyour', 'oldtopos', 'Spacetech', 'JasonKing', 'BaseballNut51', 'afdreher', 'jzhu', 'Woodbug', 'mattropolis', 'Milo', 'DavidF', 'Chris Pendleton', 'MAPconcierge', 'Gary Stebbins', 'CameronLamp', 'Woogey', 'bdiscoe', 'nadus', 'flowerpoop', 'westyvw', 'westptx', 'Artur Seidel', 'bohnenstange', 'Baugh016', 'J & B Adventures', 'HolgerJeromin', 'AndiG88', 'tlsvict', 'Pat Tressel', 'james_hiebert', 'Sheltiego', 'NWmapper', 'shravan91', 'taylorkc', 'nm7s9', '883Rrider', 'KeumpSeattle', 'wasat', 'Little Brother', 'Neuwieder', 'jakelthompson', 'Frank Krueger', 'danielki33', 'hareeshkumar', 'senapa', 'Aleks-Berlin', 'Russell926', 'thayward', 'ofnine', 'jharvey', 'Ballard OpenStreetMap', 'Yellow_Giraffe', 'yinazhu_import', 'Coz Mo', 'bbaxter94', 'Chetan_Gowda', 'TwoNineteenTrain', 'Rich Fought', 'cbeddow', 'gabis_telenav', 'PTmapper', '42429', 'eggfarkarch', 'Beau Gunderson', 'jumbanho', 'slindy23', 'seattlefyi_import', 'hawkyyy', 'kinematicdigit', 'SWF8', 'kasatch', 'Alan Trick', 'Christian Barrett', 'lokejul', 'Darrell', 'Amandalee43', 'Preferred', 'sanok', 'garciapaul95', 'The Wapiti', 'Jason Reid', 'homo11', 'annotator57', 'rgreinho', 'Lantius', 'wislander', 'nikhilprabhakar', 'diggernet', 'Biff', 'ianmcorvidae', '7465677966', 'cleversprocket', 'WSS-1', 'PatronusAnalytical', 'Jfact0ry', 'prajmus', 'niten', 'T_9er', 'NickDaKingPin', 'pratikyadav', 'love2run57', 'Lumun', 'AMY-jin', 'mcutting', 'Adam2046', 'Julian Gibson', 'Satoru Nakata', 'Kolenka', 'Redens', 'sctrojan79-import', 'Geodesy99', 'DonH', 'Jacob Lesser', 'BYUDigger', 'federali', 'hlidka', 'Seporah', 'pozine', 'BigKage', 'Grant Humphries', 'Glassman', 'Nattgew', 'alester', 'tbrownbc', 'Alan', 'Jason Anderson', 'beaver dean', 'UberHiker', 'ma-rt-in', 'manoharuss', 'mfagan', 'JasonDW', 'Rub21', 'ch166', 'LkBoren Loch Ness', 'shokonogu', 'uboot', 'emac', 'Doug Hull', 'dalmond', 'henningpohl', 'tpotter', 'Cherrybowl', 'Jeff_WSDOT', 'slhutchins', 'jllust', 'MJD98004', 'Fischtor', 'jharpster', 'VictoriaAM', 'Jay Beavers', 'DeVietor', 'mazzygos', 'Max--', 'van Rees', 'De Idstaaner', 'MasterOfKittens', 'David Hagfors', 'maxburton', 'MrShrike', 'tkmedia', 'JD_cartography', 'xunilOS', 'somehow_different', 'flffddy', 'joshuavandyk', 'maxerickson', 'EuropaUSA', 'houston_mapper1', 'sshevlyagin', 'joshuapopps', 'kammern', 'ottercove', 'Shmias', 'lagerratrobe', 'bindars', 'C-C_', 'pomtree', 'Westin Miller', 'sir-mapalot', 'bwreilly', 'michael0x2a', 'Scott Shawcroft', 'Ben Allard', 'Bill Huckabee', 'DeanGifford'}\n"
     ]
    }
   ],
   "source": [
    "# Metadata\n",
    "METADATA = [ 'version', 'changeset', 'timestamp', 'user', 'uid', 'id']\n",
    "\n",
    "\n",
    "'''\n",
    "    Reference:  Udacity\n",
    "'''\n",
    "\n",
    "def get_user(elem, users):\n",
    "    if elem.tag == \"node\":\n",
    "        uid = elem.get('user')\n",
    "        if uid not in users and uid != None:\n",
    "            users.add(uid)\n",
    "    return users\n",
    "\n",
    "\n",
    "def user_contributors(filename):\n",
    "    users = set()\n",
    "    for _, elem in xET.iterparse(filename):\n",
    "        user = get_user(elem, users)\n",
    "    return users\n",
    "\n",
    "\n",
    "def get_user_contributors():\n",
    "    users = user_contributors(SAMPLE_NAME)\n",
    "    print ('Total Users: ', len(users))\n",
    "    print(users)\n",
    "    \n",
    "get_user_contributors()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this sample set, there were 1558 unique contributors. When there are so many cooks in the kitchen, data is bound to have formatting issues, as well as inconsistent taxonomy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Audit Street Addresses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the list of tag elements that contain field data, there is a lot of information that could be generated from this data set. Since we're working with map data, addresses will be one of the most important pieces of information we will wrangle. Part of the process for validating addresses will be checking for uniformity and consistency amongst commonly used street types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Zipcodes:  144\n",
      "Invalid Zipcodes:  20\n",
      "{'Olympia, 98501': 1,\n",
      " 'V8N 3E2': 1,\n",
      " 'V8P 2P4': 1,\n",
      " 'V8R 5V6': 1,\n",
      " 'V8S2J8': 1,\n",
      " 'V8T 1G1': 1,\n",
      " 'V8T4K7': 1,\n",
      " 'V8W 1H6': 1,\n",
      " 'V8Y 3H1': 1,\n",
      " 'V8Z6E4': 1,\n",
      " 'V8Z6E6': 1,\n",
      " 'V9A 6N7': 1,\n",
      " 'V9A 7N6': 1,\n",
      " 'V9B 1H2': 1,\n",
      " 'V9B 1R6': 1,\n",
      " 'V9B1L8': 2,\n",
      " 'V9B1V7': 1,\n",
      " 'V9Z 1B2': 1,\n",
      " 'v8Z 1H1': 1,\n",
      " 'v8r 5e9': 1}\n",
      "Total Ways:  15\n",
      "{'East': {'13th Avenue East',\n",
      "          '15th Avenue East',\n",
      "          '22nd Avenue East',\n",
      "          '25th Avenue East',\n",
      "          'Belmont Place East',\n",
      "          'Boylston Avenue East',\n",
      "          'Broadmoor Drive East',\n",
      "          'Dorffel Drive East',\n",
      "          'Minor Avenue East',\n",
      "          'Yale Avenue East'},\n",
      " 'Fir': {'East Fir'},\n",
      " 'Highway': {'Patricia Bay Highway'},\n",
      " 'NE': {'161st Avenue NE'},\n",
      " 'North': {'1st Avenue North',\n",
      "           'Ashworth Avenue North',\n",
      "           'Burke Avenue North',\n",
      "           'Dayton Avenue North',\n",
      "           'Dexter Avenue North',\n",
      "           'East Green Lake Way North',\n",
      "           'Fremont Avenue North',\n",
      "           'Greenwood Avenue North',\n",
      "           'Interlake Avenue North',\n",
      "           'Linden Avenue North',\n",
      "           'Meridian Avenue North',\n",
      "           'Midvale Avenue North',\n",
      "           'Palatine Avenue North',\n",
      "           'Phinney Avenue North',\n",
      "           'Stone Avenue North'},\n",
      " 'Northeast': {'103rd Place Northeast',\n",
      "               '107th Place Northeast',\n",
      "               '10th Avenue Northeast',\n",
      "               '110th Avenue Northeast',\n",
      "               '113th Court Northeast',\n",
      "               '116th Avenue Northeast',\n",
      "               '118th Avenue Northeast',\n",
      "               '124th Avenue Northeast',\n",
      "               '127th Place Northeast',\n",
      "               '134th Avenue Northeast',\n",
      "               '137th Place Northeast',\n",
      "               '15th Avenue Northeast',\n",
      "               '15th Place Northeast',\n",
      "               '160th Place Northeast',\n",
      "               '16th Avenue Northeast',\n",
      "               '1st Avenue Northeast',\n",
      "               '20th Avenue Northeast',\n",
      "               '24th Avenue Northeast',\n",
      "               '25th Avenue Northeast',\n",
      "               '27th Avenue Northeast',\n",
      "               '2nd Avenue Northeast',\n",
      "               '32nd Avenue Northeast',\n",
      "               '33rd Avenue Northeast',\n",
      "               '36th Avenue Northeast',\n",
      "               '38th Avenue Northeast',\n",
      "               '39th Avenue Northeast',\n",
      "               '3rd Avenue Northeast',\n",
      "               '42nd Avenue Northeast',\n",
      "               '44th Place Northeast',\n",
      "               '45th Avenue Northeast',\n",
      "               '46th Avenue Northeast',\n",
      "               '47th Avenue Northeast',\n",
      "               '52nd Avenue Northeast',\n",
      "               '5th Avenue Northeast',\n",
      "               '73rd Place Northeast',\n",
      "               '93rd Avenue Northeast',\n",
      "               '96th Place Northeast',\n",
      "               '97th Lane Northeast',\n",
      "               'Brooklyn Avenue Northeast',\n",
      "               'Holmes Point Drive Northeast',\n",
      "               'Jefferson Street Northeast',\n",
      "               'Paisley Place Northeast',\n",
      "               'Ravenna Avenue Northeast',\n",
      "               'Roosevelt Way Northeast',\n",
      "               'Thackeray Place Northeast'},\n",
      " 'Northwest': {'11th Avenue Northwest',\n",
      "               '14th Avenue Northwest',\n",
      "               '15th Avenue Northwest',\n",
      "               '17th Avenue Northwest',\n",
      "               '28th Avenue Northwest',\n",
      "               '29th Avenue Northwest',\n",
      "               '2nd Avenue Northwest',\n",
      "               '32nd Avenue Northwest',\n",
      "               '5th Avenue Northwest',\n",
      "               'Ballard Avenue Northwest',\n",
      "               'Mary Avenue Northwest'},\n",
      " 'South': {'13th Avenue South',\n",
      "           '17th Avenue South',\n",
      "           '21st Avenue South',\n",
      "           '22nd Avenue South',\n",
      "           '24th Avenue South',\n",
      "           '24th Place South',\n",
      "           '25th Avenue South',\n",
      "           '28th Avenue South',\n",
      "           '2nd Avenue South',\n",
      "           '30th Avenue South',\n",
      "           '32nd Avenue South',\n",
      "           '34th Avenue South',\n",
      "           '41st Avenue South',\n",
      "           '48th Avenue South',\n",
      "           '4th Avenue South',\n",
      "           '54th Avenue South',\n",
      "           '5th Avenue South',\n",
      "           '63rd Avenue South',\n",
      "           'Airport Way South',\n",
      "           'Cascadia Avenue South',\n",
      "           'Duncan Avenue South',\n",
      "           'Poplar Place South',\n",
      "           'Renton Avenue South',\n",
      "           'Seward Park Avenue South'},\n",
      " 'Southeast': {'142nd Avenue Southeast'},\n",
      " 'Southwest': {'17th Avenue Southwest',\n",
      "               '21st Avenue Southwest',\n",
      "               '23rd Avenue Southwest',\n",
      "               '25th Avenue Southwest',\n",
      "               '26th Avenue Southwest',\n",
      "               '28th Avenue Southwest',\n",
      "               '36th Avenue Southwest',\n",
      "               '37th Avenue Southwest',\n",
      "               '39th Avenue Southwest',\n",
      "               '47th Avenue Southwest',\n",
      "               '57th Avenue Southwest',\n",
      "               '5th Avenue Southwest',\n",
      "               'Beach Drive Southwest',\n",
      "               'California Avenue Southwest',\n",
      "               'Fauntleroy Way Southwest',\n",
      "               'High Point Drive Southwest',\n",
      "               'West Marginal Way Southwest'},\n",
      " 'Ter': {'Bravo Ter'},\n",
      " 'Terrace': {'King George Terrace'},\n",
      " 'View': {'Kulshan View'},\n",
      " 'Way': {'Southwest Admiral Way', 'East Yesler Way', 'Northeast 131st Way'},\n",
      " 'West': {'10th Avenue West',\n",
      "          '11th Avenue West',\n",
      "          '12th Avenue West',\n",
      "          '15th Avenue West',\n",
      "          '1st Avenue West',\n",
      "          '21st Avenue West',\n",
      "          '23rd Avenue West',\n",
      "          '2nd Avenue West',\n",
      "          '30th Avenue West',\n",
      "          '35th Avenue West',\n",
      "          '38th Avenue West',\n",
      "          '5th Avenue West',\n",
      "          '9th Street West',\n",
      "          'Gorge Road West',\n",
      "          'Thorndyke Avenue West',\n",
      "          'Williams Avenue West'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "expected_street_types = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "    Reference:  Udacity\n",
    "'''\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_street_types:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "            \n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v=d[k]\n",
    "        print('%s: %d' % (k, v))\n",
    "\n",
    "        \n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == 'addr:street')\n",
    "\n",
    "\n",
    "\n",
    "def audit_streets():\n",
    "    for event, elem in xET.iterparse(SMALL_SAMPLE_FILE, events=('start',)):\n",
    "        if elem.tag == \"node\" or elem.tag == 'way':\n",
    "            for tag in elem.iter('tag'):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    print('Total Ways: ', len(street_types)) #Total Number of Different Types of Streets\n",
    "    pprint.pprint(dict(street_types))#List of Street Types\n",
    "    \n",
    "\n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == 'addr:postcode')\n",
    "    \n",
    "    \n",
    "def audit_zips():\n",
    "    zipcodes = {}\n",
    "    inv_zipcodes = {}\n",
    "    for event, elem in xET.iterparse(SAMPLE_FILE, events=('start',)):\n",
    "         if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag): #and tag.attrib['v'].startswith('98')\n",
    "                    zipc = tag.attrib['v']\n",
    "                    if zipc not in zipcodes.keys():\n",
    "                        zipcodes[zipc] = 1\n",
    "                    else:\n",
    "                        zipcodes[zipc] = zipcodes[zipc] + 1\n",
    "                        \n",
    "                    if not tag.attrib['v'].startswith('98'):\n",
    "                        inv_zipc = tag.attrib['v']\n",
    "                        if inv_zipc not in inv_zipcodes.keys():\n",
    "                            inv_zipcodes[inv_zipc] = 1\n",
    "                        else:\n",
    "                            inv_zipcodes[inv_zipc] = inv_zipcodes[inv_zipc] + 1\n",
    "                    #else:\n",
    "                    #    print(tag.attrib['v'])\n",
    "                    #    zipcodes[tag.attrib['v']] += 1\n",
    "    print('Total Zipcodes: ', len(zipcodes)) #Total Number of Zipcodes\n",
    "    print('Invalid Zipcodes: ', len(inv_zipcodes)) #Total Number of Invalid Zipcodes\n",
    "    pprint.pprint(dict(inv_zipcodes)) #List of Invalid Zipcodes\n",
    "\n",
    "audit_zips()\n",
    "audit_streets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset shows us how many variations there are when it comes to how users enter abbreviations for street names. This is one opportunity we'll have to clean up the information before it's parsed into JSON/CSV. Some of the main problems are:\n",
    "\n",
    "* Abbreviations for types of street and directions are both extremely inconsitent\n",
    " * We'll want to decide if abbreviations are appropriate, or if using full words is better, and then stick with one format.\n",
    "* Some addresses contain junk\n",
    " * Junk data may want to be scrubbed out if it cannot be corrected.\n",
    "* Some streets do not contain street types\n",
    " * This could be data entry errors, or by design, so looking into these individually could be important\n",
    " \n",
    "Using additional regexes will help automatically identify patterns that may be associated with problems in data.\n",
    "\n",
    "When we look at zip codes that might be invalid for Seattle (not beginning with 98-), we can see that the OSM data for the Greater Seattle Region includes parts of British Columbia, Canada. These zip codes begin with V8 or V9. Overall, this is okay since we can update the rule we're using to validate zip codes to include these Canadian zip codes. What we _will_ want to clean up is the zipcode that includes the city of Olympia, as well as ensure that zip codes for BC all contain a space in the middle, and are all caps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding Problems via Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 2306963, 'lower_colon': 2362363, 'other': 81841, 'problemchars': 3}\n"
     ]
    }
   ],
   "source": [
    "# Regexes\n",
    "REGEX_ST_TYPE = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "REGEX_LOWER = re.compile(r'^([a-z]|_)*$')\n",
    "REGEX_LOWER_COLON = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "REGEX_PROBLEMCHAR = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# Street Types in Addresses\n",
    "st_types = defaultdict(set)\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(set)\n",
    "\n",
    "expected_street_types = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "'''\n",
    "    Reference:  Udacity\n",
    "'''\n",
    "\n",
    "def key_type(elem, keys):\n",
    "    if elem.tag == \"tag\":\n",
    "        k = elem.get('k')\n",
    "        if REGEX_LOWER.search(k):# tags that contain only lowercase letters and are valid\n",
    "            if 'lower' in keys:\n",
    "                keys['lower'] += 1\n",
    "            else:\n",
    "                keys['lower'] = 1\n",
    "        elif REGEX_LOWER_COLON.search(k): # valid tags with a colon in their names\n",
    "            if 'lower_colon' in keys:\n",
    "                keys['lower_colon'] += 1\n",
    "            else:\n",
    "                keys['lower_colon'] = 1\n",
    "        elif REGEX_PROBLEMCHAR.search(k): # tags with problematic characters\n",
    "            if 'problemchars' in keys:\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['problemchars'] = 1\n",
    "        else:\n",
    "            keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def identify_problem_street_types(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, elem in ET.iterparse(filename):\n",
    "        keys = key_type(elem, keys)\n",
    "    return keys\n",
    "\n",
    "\n",
    "\n",
    "def print_problem_street_types():\n",
    "    # You can use another testfile 'map.osm' to look at your solution\n",
    "    # Note that the assertion below will be incorrect then.\n",
    "    # Note as well that the test function here is only used in the Test Run;\n",
    "    # when you submit, your code will be checked against a different dataset.\n",
    "    keys = identify_problem_street_types(OSM_FILE)\n",
    "    pprint.pprint(keys)\n",
    "\n",
    "    \n",
    "print_problem_street_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the counts from this audit shows us that there's a lot of information in the dataset that could be considered ill-formatted. Overall, there were only three street types with problem characters, however there were millions of entries for both the lower and lower_colon audits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Clean Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mapped desirable values for street types, and ensured street names matched those values before conversion to JSON\n",
    " * Added frequently occurring attributes to the shape of the data\n",
    " * Many attributes that have interesting pieces of information are very sparsely populated, so to ensure the shape of the data is consistent, any attributes that didn't have a key/value pair for a certain node were populated with None.\n",
    "* Regexes were run to validate tag format, as well as cleanup to street names to match the desired format specified in the mapping dictionary\n",
    " * Invalid tags were ignored\n",
    " * Tags with colons were parted out into their own organized dictionary by type of tag (addr and tiger)\n",
    "* Zip codes validated to be in the format of ```98xxx```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Zip Codes: \n",
      "V9B1V7 => V9B 1V7\n",
      "v8r 5e9 => V8R 5E9\n",
      "Olympia, 98501 => 98501\n",
      "V9B1L8 => V9B 1L8\n",
      "v8Z 1H1 => V8Z 1H1\n",
      "V8Z6E4 => V8Z 6E4\n",
      "V8Z6E6 => V8Z 6E6\n",
      "V8S2J8 => V8S 2J8\n",
      "V8T4K7 => V8T 4K7\n",
      "\n",
      "\n",
      "Corrected Street Names: \n",
      "Total Ways:  61\n",
      "Carnation-Duvall Rd NE => Carnation-Duvall Rd Northeast\n",
      "234th Place NE => 234th Place Northeast\n",
      "Bellevue Way NE => Bellevue Way Northeast\n",
      "236th Ave NE => 236th Ave Northeast\n",
      "University Way NE => University Way Northeast\n",
      "156th Pl NE => 156th Pl Northeast\n",
      "180th Pl NE => 180th Pl Northeast\n",
      "127th PL NE => 127th PL Northeast\n",
      "Sand Point Way NE => Sand Point Way Northeast\n",
      "155th Place NE => 155th Place Northeast\n",
      "161st Avenue NE => 161st Avenue Northeast\n",
      "Limited lane NW => Limited lane Northwest\n",
      "south 58th street => south 58th Street\n",
      "Laventure Rd => Laventure Road\n",
      "Echo Lake Rd => Echo Lake Road\n",
      "S River Rd => S River Road\n",
      "112th Ave SE => 112th Ave Southeast\n",
      "224th St SE => 224th St Southeast\n",
      "241 LN SE => 241 LN Southeast\n",
      "230 Lane SE => 230 Lane Southeast\n",
      "NE 110th St => NE 110th Street\n",
      "NE 111th St => NE 111th Street\n",
      "S Spokane St => S Spokane Street\n",
      "Pacific Avenue S => Pacific Avenue South\n",
      "72nd Street E => 72nd Street East\n",
      "Bridgeport Way W => Bridgeport Way West\n",
      "198th Avenue SW => 198th Avenue Southwest\n",
      "108th Avenue SW => 108th Avenue Southwest\n",
      "Kuper Court SW => Kuper Court Southwest\n",
      "School Land Road SW => School Land Road Southwest\n",
      "Whiterock Loop SW => Whiterock Loop Southwest\n",
      "Starlit Lane SW => Starlit Lane Southwest\n",
      "Isabella Lane SW => Isabella Lane Southwest\n",
      "197th Court SW => 197th Court Southwest\n",
      "198th Trail SW => 198th Trail Southwest\n",
      "Marble Street SW => Marble Street Southwest\n",
      "201st Avenue SW => 201st Avenue Southwest\n",
      "Northwest Gilman Blvd. => Northwest Gilman Boulevard\n",
      "NE 108th Ct => NE 108th Court\n",
      "2nd Ave => 2nd Avenue\n",
      "Bravo Ter => Bravo Terrace\n"
     ]
    }
   ],
   "source": [
    "expected_street_types = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            'AVE': 'Avenue',\n",
    "            'Ave': 'Avenue',\n",
    "            'Ave.': 'Avenue',\n",
    "            'Av.': 'Avenue',\n",
    "            'ave': 'Avenue',\n",
    "            'Blvd': 'Boulevard',\n",
    "            'Blvd.': 'Boulevard',\n",
    "            'boulevard': 'Boulevard',\n",
    "            'CT': 'Court',\n",
    "            'Ct': 'Court',\n",
    "            'Dr': 'Drive',\n",
    "            'Dr.': 'Drive',\n",
    "            'E': 'East',\n",
    "            'E.Division': 'East Division',\n",
    "            'FI': 'Fox Drive',\n",
    "            'Hwy': 'Highway',\n",
    "            'K10': 'NE 8th Street',\n",
    "            'MainStreet': 'N Main Street',\n",
    "            'N': 'North',\n",
    "            'NE': 'Northeast',\n",
    "            'NW': 'Northwest',\n",
    "            'nw': 'Northwest',\n",
    "            'PL': 'Place',\n",
    "            'Pl': 'Place',\n",
    "            'Rd': 'Road',\n",
    "            'RD': 'Road',\n",
    "            'Rd.': 'Road',\n",
    "            'S': 'South',\n",
    "            'S.': 'South',\n",
    "            'S.E.': 'Southeast',\n",
    "            'SE': 'Southeast',\n",
    "            'ST': 'Street',\n",
    "            'SW': 'Southwest',\n",
    "            'SW,': 'Southwest',\n",
    "            'Se': 'Southeast',\n",
    "            'southeast': 'Southeast',\n",
    "            'St': 'Street',\n",
    "            'st': 'Street',\n",
    "            'street': 'Street',\n",
    "            'St.': 'Street',\n",
    "            'Ter': 'Terrace',\n",
    "            'W': 'West',\n",
    "            'west': 'West',\n",
    "            'WA': '17625 140th Avenue Southeast',\n",
    "            'WA)': 'US 101',\n",
    "            'WY': 'Way'\n",
    "            }\n",
    "\n",
    "'''\n",
    "    Reference:  Udacity\n",
    "'''\n",
    "\n",
    "def street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_street_types:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\") \n",
    "            \n",
    "def print_sorted_dict(d):\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v=d[k]\n",
    "        print('%s: %d' % (k, v))\n",
    "        \n",
    "def audit_streets(osm_file):\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in xET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    n = street_type_re.search(name)\n",
    "    n = n.group()\n",
    "    for m in mapping:\n",
    "        if n == m:\n",
    "            name = name[:-len(n)] + mapping[m]\n",
    "    return name\n",
    "\n",
    "def audit_update_street_types():\n",
    "    st_types = audit_streets(open(SAMPLE_NAME, \"rb\"))\n",
    "    \n",
    "    print('Total Ways: ', len(st_types)) #Total Number of street types\n",
    "    \n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name, mapping)\n",
    "            if name != better_name:\n",
    "                #Preview updated data\n",
    "                print (name, \"=>\", better_name)\n",
    "            name = better_name\n",
    "            \n",
    "def is_zipcode(elem):\n",
    "    return (elem.attrib['k'] == 'addr:postcode')\n",
    "\n",
    "\n",
    "def incorrect_zips():\n",
    "    zipcodes = {}\n",
    "    for event, elem in xET.iterparse(SAMPLE_FILE, events=('start',)):\n",
    "         if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_zipcode(tag): #and tag.attrib['v'].startswith('98')\n",
    "                    zipc = tag.attrib['v']\n",
    "                    if zipc not in zipcodes.keys():\n",
    "                        zipcodes[zipc] = 1\n",
    "                    else:\n",
    "                        zipcodes[zipc] = zipcodes[zipc] + 1\n",
    "    return zipcodes\n",
    "    \n",
    "def correct_zipcodes():\n",
    "    for zipcode in incorrect_zips():\n",
    "        try:\n",
    "            better_zipcode = update_zip(zipcode)\n",
    "        except:\n",
    "            zipcode = zipcode.upper()\n",
    "        if zipcode != better_zipcode:\n",
    "            print (zipcode, \"=>\", better_zipcode)\n",
    "            \n",
    "def update_zip(zipcode):\n",
    "    if zipcode == \"Olympia, 98501\":\n",
    "        zipcode = \"98501\"\n",
    "    elif zipcode.startswith('V') or zipcode.startswith('v'):\n",
    "            z1 = zipcode[:3]\n",
    "            z2 = zipcode[-3:]\n",
    "            zipcode = z1 + ' ' + z2\n",
    "            zipcode = zipcode.upper()\n",
    "    return zipcode\n",
    "\n",
    "\n",
    "print(\"Corrected Zip Codes: \")\n",
    "correct_zipcodes()\n",
    "print('\\n')\n",
    "print(\"Corrected Street Names: \")\n",
    "audit_update_street_types()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Manually mapping this data for the cleanup only corrects a portion of the data, but seems to catch most of the glaring problems with street suffixes and zip codes.\n",
    "* The way the data was corrected was subjective to how I chose to input the key/value pairs.\n",
    " * As there are other attribute fields available that contain address parts, ideally these catalogued abbreviations would be referenced and used when the system validates street addresses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seattle_small_sample.json created:\n",
      "File size:  8.4 MB\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sample Shape:\n",
      "{'address': {'city': None,\n",
      "             'housenumber': None,\n",
      "             'postcode': None,\n",
      "             'state': None,\n",
      "             'street': None},\n",
      " 'amenity': None,\n",
      " 'building': None,\n",
      " 'created': {'changeset': '214775',\n",
      "             'id': '25840363',\n",
      "             'timestamp': '2007-02-12T03:19:19Z',\n",
      "             'uid': '6009',\n",
      "             'user': 'CoreyBurger',\n",
      "             'version': '1'},\n",
      " 'cuisine': None,\n",
      " 'id': '25840363',\n",
      " 'name': None,\n",
      " 'phone': None,\n",
      " 'pos': [48.4621984, -123.3248062],\n",
      " 'refs': None,\n",
      " 'shop': None,\n",
      " 'tiger': {'country': None,\n",
      "           'name_base': None,\n",
      "           'name_base_1': None,\n",
      "           'name_base_2': None,\n",
      "           'name_base_3': None,\n",
      "           'name_direction_prefix': None,\n",
      "           'name_direction_prefix_1': None,\n",
      "           'name_direction_prefix_2': None,\n",
      "           'name_direction_prefix_3': None,\n",
      "           'name_direction_suffix': None,\n",
      "           'name_direction_suffix_1': None,\n",
      "           'name_direction_suffix_2': None,\n",
      "           'name_direction_suffix_3': None,\n",
      "           'name_type': None,\n",
      "           'name_type_1': None,\n",
      "           'name_type_2': None,\n",
      "           'name_type_3': None,\n",
      "           'zip_left': None,\n",
      "           'zip_right': None},\n",
      " 'type': 'node'}\n",
      "{'address': {'city': None,\n",
      "             'housenumber': None,\n",
      "             'postcode': None,\n",
      "             'state': None,\n",
      "             'street': None},\n",
      " 'amenity': None,\n",
      " 'building': None,\n",
      " 'created': {'changeset': '48663231',\n",
      "             'id': '493368137',\n",
      "             'timestamp': '2017-05-14T01:56:46Z',\n",
      "             'uid': '5192770',\n",
      "             'user': 'Island Human',\n",
      "             'version': '1'},\n",
      " 'cuisine': None,\n",
      " 'id': '493368137',\n",
      " 'name': None,\n",
      " 'phone': None,\n",
      " 'pos': [None, None],\n",
      " 'refs': ['4852872568',\n",
      "          '4852872569',\n",
      "          '4852872570',\n",
      "          '4852872571',\n",
      "          '4852872572',\n",
      "          '4852872573',\n",
      "          '4852872574',\n",
      "          '4852872575',\n",
      "          '4852872576',\n",
      "          '4852872577',\n",
      "          '4852872578',\n",
      "          '4852872579',\n",
      "          '4852872580',\n",
      "          '4852872581',\n",
      "          '4852872582',\n",
      "          '4852872583',\n",
      "          '4852872584',\n",
      "          '4852872568'],\n",
      " 'shop': None,\n",
      " 'tiger': {'country': None,\n",
      "           'name_base': None,\n",
      "           'name_base_1': None,\n",
      "           'name_base_2': None,\n",
      "           'name_base_3': None,\n",
      "           'name_direction_prefix': None,\n",
      "           'name_direction_prefix_1': None,\n",
      "           'name_direction_prefix_2': None,\n",
      "           'name_direction_prefix_3': None,\n",
      "           'name_direction_suffix': None,\n",
      "           'name_direction_suffix_1': None,\n",
      "           'name_direction_suffix_2': None,\n",
      "           'name_direction_suffix_3': None,\n",
      "           'name_type': None,\n",
      "           'name_type_1': None,\n",
      "           'name_type_2': None,\n",
      "           'name_type_3': None,\n",
      "           'zip_left': None,\n",
      "           'zip_right': None},\n",
      " 'type': 'way'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    Reference:  Udacity\n",
    "'''\n",
    "tiger = {}\n",
    "\n",
    "def file_size(SMALL_SAMPLE_NAME):\n",
    "    \"\"\"\n",
    "    this function will return the file size\n",
    "    \n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/2104080/how-to-check-file-size-in-python\n",
    "    \"\"\"\n",
    "    file_info = os.stat(SMALL_SAMPLE_NAME)\n",
    "    size = convert_bytes(file_info.st_size)\n",
    "    return size\n",
    "\n",
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/2104080/how-to-check-file-size-in-python\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            size = \"%3.1f %s\" % (num, x)\n",
    "            return size\n",
    "        num /= 1024.0\n",
    "\n",
    "def tag_attributes(f):\n",
    "    attribs = []\n",
    "    for ev, elem in xET.iterparse(f):\n",
    "        if elem.tag == 'way' or elem.tag == 'node': # Specify parent tag to filter nested tag list\n",
    "            for t in elem.iter('tag'):\n",
    "            #Iterate through tag types, adding new ones to the set for the parent tag\n",
    "                if t.attrib['k'] not in attribs:\n",
    "                    if REGEX_LOWER.search(t.attrib['k']): # Only add values for lowercase attributes\n",
    "                        attribs.append(t.attrib['k'])\n",
    "    return attribs\n",
    "\n",
    "#attributes = tag_attributes(SMALL_SAMPLE_NAME)\n",
    "#pprint.pprint(sorted(attributes))\n",
    "\n",
    "def shape_element(element):\n",
    "    node = {\n",
    "        \"id\": None, \n",
    "        \"type\": None,\n",
    "        'name': None,\n",
    "        'amenity': None,\n",
    "        'building' : None,\n",
    "        'shop' : None,\n",
    "        'cuisine' : None,\n",
    "        'phone' : None,\n",
    "        \"created\": {\n",
    "            \"changeset\": None, \n",
    "            \"user\": None, \n",
    "            \"version\": None, \n",
    "            \"uid\": None, \n",
    "            \"timestamp\": None\n",
    "        },\n",
    "        \"pos\": [None, None],\n",
    "        \"refs\": [None],\n",
    "        \"address\": {\n",
    "                  \"housenumber\": None,\n",
    "                  \"postcode\": None,\n",
    "                  \"street\": None,\n",
    "                  \"state\": None,\n",
    "                  \"city\": None,\n",
    "                },\n",
    "        \"tiger\": { \n",
    "                 'country': None,\n",
    "                 'name_base': None,\n",
    "                 'name_base_1': None,\n",
    "                 'name_base_2': None,\n",
    "                 'name_base_3': None,\n",
    "                 'name_direction_prefix': None,\n",
    "                 'name_direction_prefix_1': None,\n",
    "                 'name_direction_prefix_2': None,\n",
    "                 'name_direction_prefix_3': None,\n",
    "                 'name_direction_suffix': None,\n",
    "                 'name_direction_suffix_1': None,\n",
    "                 'name_direction_suffix_2': None,\n",
    "                 'name_direction_suffix_3': None,\n",
    "                 'name_type': None,\n",
    "                 'name_type_1': None,\n",
    "                 'name_type_2': None,\n",
    "                 'name_type_3': None,\n",
    "                 'zip_left': None,\n",
    "                 'zip_right': None,\n",
    "                }\n",
    "        }       \n",
    "    refs = []\n",
    "\n",
    "    if element.tag == \"node\" or element.tag == \"way\":\n",
    "        node['id'] = element.attrib['id'] # Get node ID\n",
    "        node['type'] = element.tag        # Get node type (node or way)\n",
    "        \n",
    "        if 'lat' in element.attrib:       # Get node position (lat/lon)\n",
    "            node['pos'] = [ast.literal_eval(element.attrib['lat']), \n",
    "                           ast.literal_eval(element.attrib['lon'])]\n",
    "            \n",
    "        for m in METADATA:                # Get 'created' metadata\n",
    "            if m in element.attrib:\n",
    "                node['created'][m] = element.attrib[m]\n",
    "        \n",
    "        for nd in element.iter('nd'):     # Iterate through nodes references\n",
    "            refs.append(nd.attrib['ref'])\n",
    "        if refs != []:\n",
    "            node['refs'] = refs\n",
    "        else:\n",
    "            node['refs'] = None\n",
    "            \n",
    "        for a in element.iter('tag'):             # Check each child tag for attributes\n",
    "            k = a.attrib['k']\n",
    "            if not(REGEX_PROBLEMCHAR.search(k)):  # Filter out tags with invalid characters\n",
    "                if k.startswith('addr:'):\n",
    "                    el = k.split(':')\n",
    "                    if el[1] == 'street':\n",
    "                        clean_street = update_name(a.attrib['v'], mapping) #Clean Street Types\n",
    "                        node['address'][el[1]] = clean_street\n",
    "                    elif el[1] == 'postcode':\n",
    "                        cleanzip = update_zip(a.attrib['v'])\n",
    "                        node['address'][el[1]] = cleanzip\n",
    "                    else:\n",
    "                        node['address'][el[1]] = a.attrib['v'] # Add other address parts\n",
    "                elif k.startswith('tiger:'):      # Get TIGER address info\n",
    "                    el = k.split(':')\n",
    "                    if el[1] == 'name_type':\n",
    "                        clean_type = update_name(a.attrib['v'], mapping) #Clean Street Types\n",
    "                        node['tiger'][el[1]] = clean_type\n",
    "                    elif el[1] in node['tiger'].keys():\n",
    "                        node['tiger'][el[1]] = a.attrib['v'] # Add other tiger parts\n",
    "                else:\n",
    "                    if k in node.keys():\n",
    "                        node[k] = a.attrib['v'] # Add key/value for all other attributes found  \n",
    "                        \n",
    "                #if node['tiger']['name_type'] != None:\n",
    "                #    pprint.pprint(node)\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def update_zip(zipcode):\n",
    "    if zipcode == \"Olympia, 98501\":\n",
    "        zipcode = \"98501\"\n",
    "    elif zipcode.startswith('V') or zipcode.startswith('v'):\n",
    "            z1 = zipcode[:3]\n",
    "            z2 = zipcode[-3:]\n",
    "            zipcode = z1 + ' ' + z2\n",
    "            zipcode = zipcode.upper()\n",
    "    return zipcode\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    n = street_type_re.search(name)\n",
    "    n = n.group()\n",
    "    for m in mapping:\n",
    "        if n == m:\n",
    "            name = name[:-len(n)] + mapping[m]\n",
    "    return name    \n",
    "    \n",
    "def process_map(file_in, pretty = False):\n",
    "    json_file = file_in + \".json\"\n",
    "  \n",
    "    data = []\n",
    "    with codecs.open(json_file, \"w\") as fo:\n",
    "        for _, element in xET.iterparse(file_in + '.osm'):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                data.append(el)\n",
    "                if pretty:\n",
    "                    fo.write(json.dumps(el, indent=2)+\"\\n\")\n",
    "                else:\n",
    "                    fo.write(json.dumps(el) + \"\\n\")\n",
    "    \n",
    "    #file_info = os.stat(json_file)\n",
    "    #size = convert_bytes(file_info.st_size)\n",
    "\n",
    "    return data\n",
    "\n",
    "def shape_data():\n",
    "    file_name = SMALL_SAMPLE_NAME.split('.')\n",
    "    json_file = file_name[0] + \".json\"\n",
    "    \n",
    "    data = process_map(file_name[0], False)\n",
    "    \n",
    "    print(json_file, 'created:')\n",
    "    print('File size: ', file_size(json_file))\n",
    "    print ('\\n\\n\\n')\n",
    "    print('Sample Shape:')    \n",
    "    pprint.pprint(data[1])\n",
    "    #pprint.pprint(data[5020:5240])\n",
    "    pprint.pprint(data[-1])\n",
    "    \n",
    "shape_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many attributes that have interesting pieces of information are very sparsely populated, so to ensure the shape of the data is consistent, any attributes that didn't have a key/value pair for a certain node were populated with ```None```.\n",
    "\n",
    "Adding these extra key/value/null sets makes the size of the cleaned and shaped JSON file data larger than the size of the original OSM Sample file.\n",
    "\n",
    "Additionally, when the data was staged for the JSON file, regexes were run to validate tag format, as well as cleanup to street names to match the desired format specified in the mapping dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data in MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the exploration that was done in the initial phases is made much simpler once the data is cleaned up and imported into MongoDB. Additionally MongoDB will enable us to take a much more in depth look at the information with a lot less code.\n",
    "\n",
    "We'll look at some of the previously scrapped data points to see how they compare when queried via pymongo now that we've normalized the shape of the most interesting pieces of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x201693d88b8>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reference:  Udacity\n",
    "'''\n",
    "file_name = SAMPLE_NAME.split('.')\n",
    "\n",
    "data = process_map(file_name[0], False)\n",
    "client = MongoClient()\n",
    "db = client.SeattleOSM\n",
    "collection = db.Sample\n",
    "collection.insert_many(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'SeattleOSM'), 'Sample')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### File Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML File:  55.321757316589355 Mb\n",
      "JSON File:  246.0293664932251 Mb\n"
     ]
    }
   ],
   "source": [
    "json_file = file_name[0] + \".json\"\n",
    "\n",
    "#Original XML Sample File Size\n",
    "print('XML File: ', str(os.path.getsize(SAMPLE_NAME)/1024/1024), 'Mb')\n",
    "\n",
    "#JSON File Size\n",
    "print('JSON File: ', str(os.path.getsize(json_file)/1024/1024), 'Mb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple command takes something previously coded out in it's own function, and brings it down to one line that gives us the size of the files we're working with in Mb.\n",
    "\n",
    "This data aligns with the observations earlier, but the code is much more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Nodes and Ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1030656\n",
      "Number of ways:   102036\n",
      "Total entries:   1132692\n"
     ]
    }
   ],
   "source": [
    "print('Number of nodes:', collection.find({\"type\":\"node\"}).count())\n",
    "print('Number of ways:  ', collection.find({\"type\":\"way\"}).count())\n",
    "print('Total entries:  ', collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Unique Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Contributors:  1677\n"
     ]
    }
   ],
   "source": [
    "# Number of unique users\n",
    "print('Unique Contributors: ', len(db.Sample.distinct(\"created.uid\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 10 Contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 User Contributors: \n",
      "[{'_id': 'Glassman', 'count': 171028},\n",
      " {'_id': 'SeattleImport', 'count': 98436},\n",
      " {'_id': 'tylerritchie', 'count': 87804},\n",
      " {'_id': 'woodpeck_fixbot', 'count': 78332},\n",
      " {'_id': 'alester', 'count': 48320},\n",
      " {'_id': 'Omnific', 'count': 36588},\n",
      " {'_id': 'Glassman_Import', 'count': 30096},\n",
      " {'_id': 'STBrenden', 'count': 28616},\n",
      " {'_id': 'CarniLvr79', 'count': 28420},\n",
      " {'_id': 'Brad Meteor', 'count': 23244}]\n"
     ]
    }
   ],
   "source": [
    "pl = [{\"$group\":{\"_id\": \"$created.user\",\n",
    "                 \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}},\n",
    "            {\"$limit\": 10}]\n",
    "result = list(collection.aggregate(pl))\n",
    "print('Top 10 User Contributors: ')\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Count Non-Null Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: 1132692\n",
      "name: 29400\n",
      "amenity: 5492\n",
      "building: 43672\n",
      "shop: 1452\n",
      "phone: 484\n",
      "pos: 1030656\n"
     ]
    }
   ],
   "source": [
    "fields = ['id',\n",
    "     'name',\n",
    "     'amenity',\n",
    "     'building',\n",
    "     'shop',\n",
    "     'phone',\n",
    "     'pos']\n",
    "\n",
    "for field in fields: #Iterates through basic field values to count non-null occurances of each\n",
    "    print(field + ':', db.Sample.find({field:{\"$ne\": None}}).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types of Buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of Building Types: \n",
      "[{'_id': None, 'count': 1089020},\n",
      " {'_id': 'yes', 'count': 38008},\n",
      " {'_id': 'house', 'count': 2448},\n",
      " {'_id': 'residential', 'count': 1476},\n",
      " {'_id': 'apartments', 'count': 368},\n",
      " {'_id': 'commercial', 'count': 244},\n",
      " {'_id': 'school', 'count': 152},\n",
      " {'_id': 'garage', 'count': 132},\n",
      " {'_id': 'retail', 'count': 116},\n",
      " {'_id': 'roof', 'count': 112},\n",
      " {'_id': 'mobile_home', 'count': 104},\n",
      " {'_id': 'shed', 'count': 64},\n",
      " {'_id': 'terrace', 'count': 56},\n",
      " {'_id': 'industrial', 'count': 52},\n",
      " {'_id': 'carport', 'count': 40},\n",
      " {'_id': 'detached', 'count': 40},\n",
      " {'_id': 'university', 'count': 28},\n",
      " {'_id': 'garages', 'count': 28},\n",
      " {'_id': 'church', 'count': 24},\n",
      " {'_id': 'warehouse', 'count': 20},\n",
      " {'_id': 'hangar', 'count': 20},\n",
      " {'_id': 'public', 'count': 16},\n",
      " {'_id': 'office', 'count': 16},\n",
      " {'_id': 'static_caravan', 'count': 12},\n",
      " {'_id': 'dormitory', 'count': 12},\n",
      " {'_id': 'greenhouse', 'count': 12},\n",
      " {'_id': 'college', 'count': 8},\n",
      " {'_id': 'cabin', 'count': 8},\n",
      " {'_id': 'kiosk', 'count': 4},\n",
      " {'_id': 'barn', 'count': 4},\n",
      " {'_id': 'hut', 'count': 4},\n",
      " {'_id': 'construction', 'count': 4},\n",
      " {'_id': 'supermarket', 'count': 4},\n",
      " {'_id': 'storage', 'count': 4},\n",
      " {'_id': 'bunker', 'count': 4},\n",
      " {'_id': 'no', 'count': 4},\n",
      " {'_id': 'service', 'count': 4},\n",
      " {'_id': 'hospital', 'count': 4},\n",
      " {'_id': 'data_center', 'count': 4},\n",
      " {'_id': 'farm_auxiliary', 'count': 4},\n",
      " {'_id': 'floating_home', 'count': 4},\n",
      " {'_id': 'entrance', 'count': 4}]\n"
     ]
    }
   ],
   "source": [
    "pl = [{\"$group\":{\"_id\": \"$building\",\n",
    "                 \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}]\n",
    "result = list(collection.aggregate(pl))\n",
    "print('Counts of Building Types: ')\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts of Shop Types: \n",
      "[{'_id': None, 'count': 1131240},\n",
      " {'_id': 'convenience', 'count': 212},\n",
      " {'_id': 'car_repair', 'count': 116},\n",
      " {'_id': 'hairdresser', 'count': 84},\n",
      " {'_id': 'beauty', 'count': 80},\n",
      " {'_id': 'clothes', 'count': 80},\n",
      " {'_id': 'supermarket', 'count': 72},\n",
      " {'_id': 'car', 'count': 64},\n",
      " {'_id': 'yes', 'count': 52},\n",
      " {'_id': 'mobile_phone', 'count': 44},\n",
      " {'_id': 'pet', 'count': 36},\n",
      " {'_id': 'dry_cleaning', 'count': 28},\n",
      " {'_id': 'furniture', 'count': 28},\n",
      " {'_id': 'car_parts', 'count': 24},\n",
      " {'_id': 'garden_centre', 'count': 20},\n",
      " {'_id': 'tobacco', 'count': 16},\n",
      " {'_id': 'massage', 'count': 16},\n",
      " {'_id': 'bicycle', 'count': 16},\n",
      " {'_id': 'department_store', 'count': 16},\n",
      " {'_id': 'hardware', 'count': 16},\n",
      " {'_id': 'greengrocer', 'count': 12},\n",
      " {'_id': 'confectionery', 'count': 12},\n",
      " {'_id': 'shoes', 'count': 12},\n",
      " {'_id': 'tattoo', 'count': 12},\n",
      " {'_id': 'vacant', 'count': 12},\n",
      " {'_id': 'bakery', 'count': 12},\n",
      " {'_id': 'antiques', 'count': 12},\n",
      " {'_id': 'outdoor', 'count': 12},\n",
      " {'_id': 'toys', 'count': 12},\n",
      " {'_id': 'second_hand', 'count': 12},\n",
      " {'_id': 'art', 'count': 12},\n",
      " {'_id': 'doityourself', 'count': 12},\n",
      " {'_id': 'jewelry', 'count': 12},\n",
      " {'_id': 'tyres', 'count': 12},\n",
      " {'_id': 'variety_store', 'count': 8},\n",
      " {'_id': 'mall', 'count': 8},\n",
      " {'_id': 'optician', 'count': 8},\n",
      " {'_id': 'gift', 'count': 8},\n",
      " {'_id': 'pawnbroker', 'count': 8},\n",
      " {'_id': 'stationery', 'count': 8},\n",
      " {'_id': 'tea', 'count': 8},\n",
      " {'_id': 'sports', 'count': 8},\n",
      " {'_id': 'florist', 'count': 8},\n",
      " {'_id': 'musical_instrument', 'count': 8},\n",
      " {'_id': 'cannabis', 'count': 8},\n",
      " {'_id': 'chemist', 'count': 8},\n",
      " {'_id': 'deli', 'count': 8},\n",
      " {'_id': 'no', 'count': 4},\n",
      " {'_id': 'ticket', 'count': 4},\n",
      " {'_id': 'hobby', 'count': 4},\n",
      " {'_id': 'fabric', 'count': 4},\n",
      " {'_id': 'scuba_diving', 'count': 4},\n",
      " {'_id': 'tanning', 'count': 4},\n",
      " {'_id': 'craft', 'count': 4},\n",
      " {'_id': 'funeral_directors', 'count': 4},\n",
      " {'_id': 'storage_rental', 'count': 4},\n",
      " {'_id': 'farm', 'count': 4},\n",
      " {'_id': 'laundry', 'count': 4},\n",
      " {'_id': 'music', 'count': 4},\n",
      " {'_id': 'frame', 'count': 4},\n",
      " {'_id': 'nutritional_supplements', 'count': 4},\n",
      " {'_id': 'sign', 'count': 4},\n",
      " {'_id': 'perfumery', 'count': 4},\n",
      " {'_id': 'alcohol', 'count': 4},\n",
      " {'_id': 'money_lender', 'count': 4},\n",
      " {'_id': 'computer', 'count': 4},\n",
      " {'_id': 'cash', 'count': 4},\n",
      " {'_id': 'electronics', 'count': 4},\n",
      " {'_id': 'weapons', 'count': 4},\n",
      " {'_id': 'marijuana', 'count': 4},\n",
      " {'_id': 'home_furnishings', 'count': 4},\n",
      " {'_id': 'estate_agent', 'count': 4},\n",
      " {'_id': 'travel_agency', 'count': 4},\n",
      " {'_id': 'books', 'count': 4},\n",
      " {'_id': 'video_games', 'count': 4},\n",
      " {'_id': 'housewares', 'count': 4},\n",
      " {'_id': 'religion', 'count': 4},\n",
      " {'_id': 'medical_marijuana', 'count': 4},\n",
      " {'_id': 'carpet', 'count': 4},\n",
      " {'_id': 'studio', 'count': 4},\n",
      " {'_id': 'copyshop', 'count': 4},\n",
      " {'_id': 'butcher', 'count': 4},\n",
      " {'_id': 'coin', 'count': 4},\n",
      " {'_id': 'aesthics', 'count': 4},\n",
      " {'_id': 'self_storage', 'count': 4},\n",
      " {'_id': 'wine', 'count': 4},\n",
      " {'_id': 'crafts', 'count': 4}]\n"
     ]
    }
   ],
   "source": [
    "pl = [{\"$group\":{\"_id\": \"$shop\",\n",
    "                 \"count\": {\"$sum\": 1}}},\n",
    "            {\"$sort\": {\"count\": -1}}]\n",
    "result = list(collection.aggregate(pl))\n",
    "print('Counts of Shop Types: ')\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Popular Cuisines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Popular Types of Food: \n",
      "[{'_id': 'pizza', 'count': 48},\n",
      " {'_id': 'mexican', 'count': 44},\n",
      " {'_id': 'chinese', 'count': 24},\n",
      " {'_id': 'japanese', 'count': 20},\n",
      " {'_id': 'asian', 'count': 20},\n",
      " {'_id': 'italian', 'count': 16},\n",
      " {'_id': 'burger', 'count': 16},\n",
      " {'_id': 'american', 'count': 16},\n",
      " {'_id': 'indian', 'count': 16},\n",
      " {'_id': 'thai', 'count': 12}]\n"
     ]
    }
   ],
   "source": [
    "pl = [{\"$match\": {\"amenity\":\"restaurant\", \n",
    "                  \"cuisine\": {\"$ne\":None}}}, \n",
    "            {\"$group\":{\"_id\":\"$cuisine\", \n",
    "                       \"count\":{\"$sum\":1}}},        \n",
    "            {\"$sort\":{\"count\":-1}}, \n",
    "            {\"$limit\":10}]\n",
    "result = list(collection.aggregate(pl))\n",
    "print('Most Popular Types of Food: ')\n",
    "pprint.pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems and Challenges with Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The investigation done on this dataset is based on limited understanding of the overall structure behind the OpenStreetMaps data. \n",
    "* To avoid making assumptions about the meaning or connections between obscure information, this audit chose to focus on cleanup of data with obvious meanings, such as address and user information.\n",
    "* Many tags are inconsistent and appear to be subjectively entered by users, so getting them uniform would require a large amount of investigatory work.\n",
    "* Data points show up unders more than one tag, so it's possible for some counts have have duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I were wrangling this data for a more structured database, I would take advantage of the parted out date information that is contained within the ```tiger``` fields, and use that to cross check and correct ```addr``` information for each node/way\n",
    "\n",
    "Additionally, I would include all attributes for each item being transfered to JSON, however this requires keeping track of a LOT of additional data, so for the purposes of this audit, only the most interesting fields were included in the transfer to MongoDB.\n",
    "\n",
    "Finally, there is a lot of inconsistency between how attributes are used and assigned, so having a more regulated and well documented system detailing how users are expected to ideally enter descritive data about things would be very beneficial to being able to study the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
